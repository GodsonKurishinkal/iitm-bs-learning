{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Covariance, Correlation & Independence\n",
    "\n",
    "**Course:** Statistics for Data Science II (BSMA1004)  \n",
    "**Week:** 2 of 12\n",
    "\n",
    "## Learning Objectives\n",
    "- Master covariance and correlation concepts\n",
    "- Understand independence vs uncorrelated variables\n",
    "- Apply to feature selection and multicollinearity detection\n",
    "- Implement correlation analysis in Python\n",
    "- Interpret correlation matrices for real datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print('‚úì Libraries loaded successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Review: Independence\n",
    "\n",
    "### Mathematical Definition\n",
    "X and Y are **independent** if:\n",
    "$$P(X \\in A, Y \\in B) = P(X \\in A) \\cdot P(Y \\in B)$$\n",
    "\n",
    "**Equivalently:**\n",
    "- Discrete: $p_{X,Y}(x,y) = p_X(x) \\cdot p_Y(y)$ for all x,y\n",
    "- Continuous: $f_{X,Y}(x,y) = f_X(x) \\cdot f_Y(y)$ for all x,y\n",
    "- Conditional: $p_{Y|X}(y|x) = p_Y(y)$ (Y doesn't depend on X)\n",
    "\n",
    "**Key Property:** Independence ‚áí Zero Covariance (but NOT vice versa!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test independence function\n",
    "def test_independence(joint_pmf, marginal_x, marginal_y):\n",
    "    \"\"\"Test if P(X,Y) = P(X)P(Y) for all cells\"\"\"\n",
    "    expected = np.outer(marginal_x, marginal_y)\n",
    "    return np.allclose(joint_pmf, expected)\n",
    "\n",
    "# Independent case: two fair coin flips\n",
    "joint_indep = np.array([\n",
    "    [0.25, 0.25],\n",
    "    [0.25, 0.25]\n",
    "])\n",
    "\n",
    "marginal_X = joint_indep.sum(axis=1)  # [0.5, 0.5]\n",
    "marginal_Y = joint_indep.sum(axis=0)  # [0.5, 0.5]\n",
    "\n",
    "print(f\"Independent? {test_independence(joint_indep, marginal_X, marginal_Y)}\")\n",
    "# True\n",
    "\n",
    "# Dependent case\n",
    "joint_dep = np.array([\n",
    "    [0.3, 0.2],\n",
    "    [0.1, 0.4]\n",
    "])\n",
    "marginal_X_dep = joint_dep.sum(axis=1)\n",
    "marginal_Y_dep = joint_dep.sum(axis=0)\n",
    "\n",
    "print(f\"Dependent? {not test_independence(joint_dep, marginal_X_dep, marginal_Y_dep)}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.heatmap(joint_indep, annot=True, cmap='Greens', ax=axes[0], cbar=False)\n",
    "axes[0].set_title('Independent Distribution', fontweight='bold')\n",
    "sns.heatmap(joint_dep, annot=True, cmap='Reds', ax=axes[1], cbar=False)\n",
    "axes[1].set_title('Dependent Distribution', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Covariance\n",
    "\n",
    "### Definition\n",
    "**Covariance** measures how two variables vary together:\n",
    "\n",
    "$$\\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]$$\n",
    "\n",
    "### Interpretation\n",
    "- **Cov(X,Y) > 0**: Positive association (X‚Üë ‚Üí Y‚Üë)\n",
    "- **Cov(X,Y) < 0**: Negative association (X‚Üë ‚Üí Y‚Üì)\n",
    "- **Cov(X,Y) = 0**: No linear relationship (uncorrelated)\n",
    "\n",
    "### Properties\n",
    "1. $\\text{Cov}(X, X) = \\text{Var}(X)$\n",
    "2. $\\text{Cov}(X, Y) = \\text{Cov}(Y, X)$ (symmetric)\n",
    "3. $\\text{Cov}(aX, bY) = ab \\cdot \\text{Cov}(X, Y)$\n",
    "4. $\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) + 2\\text{Cov}(X,Y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: study hours vs exam scores\n",
    "study_hours = np.array([2, 3, 4, 5, 6, 7, 8])\n",
    "exam_scores = np.array([65, 70, 75, 80, 85, 90, 95])\n",
    "\n",
    "# Method 1: Manual calculation\n",
    "mean_hours = study_hours.mean()\n",
    "mean_scores = exam_scores.mean()\n",
    "cov_manual = np.mean((study_hours - mean_hours) * (exam_scores - mean_scores))\n",
    "print(f\"Covariance (manual): {cov_manual:.2f}\")\n",
    "\n",
    "# Method 2: Using numpy\n",
    "cov_matrix = np.cov(study_hours, exam_scores, bias=True)\n",
    "cov_numpy = cov_matrix[0, 1]\n",
    "print(f\"Covariance (numpy): {cov_numpy:.2f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\n‚úì Positive covariance ‚Üí more study hours associated with higher scores\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(study_hours, exam_scores, s=100, alpha=0.6)\n",
    "plt.plot(study_hours, exam_scores, '--', alpha=0.3)\n",
    "plt.xlabel('Study Hours', fontsize=12)\n",
    "plt.ylabel('Exam Scores', fontsize=12)\n",
    "plt.title(f'Study Hours vs Exam Scores (Cov = {cov_numpy:.2f})', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Correlation Coefficient\n",
    "\n",
    "### Definition\n",
    "**Pearson Correlation Coefficient** (normalized covariance):\n",
    "\n",
    "$$\\rho_{X,Y} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y} = \\frac{\\text{Cov}(X, Y)}{\\sqrt{\\text{Var}(X) \\text{Var}(Y)}}$$\n",
    "\n",
    "### Properties\n",
    "1. **Range**: $-1 \\leq \\rho \\leq 1$\n",
    "2. **Scale-invariant**: Changing units doesn't change œÅ\n",
    "3. **Perfect correlation**:\n",
    "   - œÅ = 1: Perfect positive linear (Y = aX + b, a > 0)\n",
    "   - œÅ = -1: Perfect negative linear (Y = aX + b, a < 0)\n",
    "   - œÅ = 0: No linear relationship\n",
    "\n",
    "### Why Normalize?\n",
    "Covariance is scale-dependent:\n",
    "- Cov(height in cm, weight in kg) ‚â† Cov(height in m, weight in g)\n",
    "- Correlation solves this: always between -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation\n",
    "correlation = np.corrcoef(study_hours, exam_scores)[0, 1]\n",
    "print(f\"Correlation: {correlation:.4f}\")\n",
    "print(f\"Interpretation: Very strong positive relationship (œÅ ‚âà 1.00)\")\n",
    "\n",
    "# Different correlation scenarios\n",
    "np.random.seed(42)\n",
    "n = 100\n",
    "\n",
    "# Create 4 scenarios\n",
    "x1 = np.linspace(0, 10, n)\n",
    "y1 = 2 * x1 + 5  # Perfect positive\n",
    "\n",
    "x2 = np.random.normal(0, 1, n)\n",
    "y2 = 0.8 * x2 + np.random.normal(0, 0.3, n)  # Strong positive\n",
    "\n",
    "x3 = np.random.normal(0, 1, n)\n",
    "y3 = np.random.normal(0, 1, n)  # No correlation\n",
    "\n",
    "x4 = np.random.normal(0, 1, n)\n",
    "y4 = -0.9 * x4 + np.random.normal(0, 0.2, n)  # Strong negative\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "scenarios = [(x1, y1, 'Perfect Positive'), (x2, y2, 'Strong Positive'),\n",
    "             (x3, y3, 'No Correlation'), (x4, y4, 'Strong Negative')]\n",
    "colors = ['blue', 'green', 'gray', 'red']\n",
    "\n",
    "for ax, (x, y, title), color in zip(axes.flat, scenarios, colors):\n",
    "    corr = np.corrcoef(x, y)[0, 1]\n",
    "    ax.scatter(x, y, alpha=0.6, color=color, s=30)\n",
    "    ax.set_title(f'{title}\\n(œÅ = {corr:.3f})', fontweight='bold')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Independence vs Uncorrelation\n",
    "\n",
    "### Critical Distinction\n",
    "\n",
    "| Property | Independence | Zero Correlation |\n",
    "|----------|-------------|------------------|\n",
    "| Definition | P(X,Y) = P(X)P(Y) | Cov(X,Y) = 0 |\n",
    "| Meaning | Complete independence | No LINEAR relationship |\n",
    "| Strength | Stronger condition | Weaker condition |\n",
    "| Direction | **Independence ‚áí Zero Cov** | **Zero Cov ‚áè Independence** |\n",
    "\n",
    "### Classic Example: Y = X¬≤\n",
    "- X ~ Uniform(-2, 2)\n",
    "- Y = X¬≤\n",
    "- Result: Cov(X,Y) = 0 BUT Y completely depends on X!\n",
    "- Reason: Symmetry cancels linear relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y = X¬≤ example\n",
    "np.random.seed(42)\n",
    "X = np.random.uniform(-2, 2, 1000)\n",
    "Y = X**2\n",
    "\n",
    "# Check correlation\n",
    "corr = np.corrcoef(X, Y)[0, 1]\n",
    "print(f\"Correlation between X and Y=X¬≤: {corr:.4f}\")\n",
    "print(f\"\\n‚ö†Ô∏è Nearly ZERO correlation, but Y is 100% determined by X!\")\n",
    "print(f\"This proves: Zero Correlation ‚â† Independence\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(X, Y, alpha=0.5, s=20)\n",
    "axes[0].set_xlabel('X', fontsize=12)\n",
    "axes[0].set_ylabel('Y = X¬≤', fontsize=12)\n",
    "axes[0].set_title(f'Uncorrelated (œÅ = {corr:.3f}) but DEPENDENT', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Show symmetry\n",
    "axes[1].scatter(X, Y, c=np.sign(X), cmap='RdBu', alpha=0.6, s=20)\n",
    "axes[1].set_xlabel('X', fontsize=12)\n",
    "axes[1].set_ylabel('Y = X¬≤', fontsize=12)\n",
    "axes[1].set_title('Symmetry Cancels Linear Relationship', fontsize=13, fontweight='bold')\n",
    "axes[1].axvline(0, color='black', linestyle='--', alpha=0.5)\n",
    "plt.colorbar(axes[1].collections[0], ax=axes[1], label='Sign of X')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Lesson: Always visualize data! Correlation only measures LINEAR relationships.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Correlation Matrix Analysis\n",
    "\n",
    "### Real Dataset Application\n",
    "Correlation matrices are essential for:\n",
    "- Feature selection\n",
    "- Multicollinearity detection\n",
    "- Understanding data relationships\n",
    "- Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create realistic dataset\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'height_cm': np.random.normal(170, 10, n),\n",
    "    'age': np.random.randint(18, 65, n),\n",
    "    'exercise_hrs': np.random.exponential(2, n)\n",
    "})\n",
    "\n",
    "# Add correlated variables\n",
    "df['weight_kg'] = 0.5 * df['height_cm'] + np.random.normal(0, 5, n)\n",
    "df['income'] = 800 * df['age'] + np.random.normal(0, 10000, n)\n",
    "df['bmi'] = df['weight_kg'] / (df['height_cm']/100)**2\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "print(\"üìä Correlation Matrix:\")\n",
    "print(corr_matrix.round(3))\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "            center=0, vmin=-1, vmax=1, square=True, linewidths=1,\n",
    "            cbar_kws={'label': 'Correlation'})\n",
    "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find strong correlations\n",
    "print(\"\\nüîç Strong Correlations (|œÅ| > 0.5):\")\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        corr_val = corr_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.5:\n",
    "            print(f\"  {corr_matrix.columns[i]} ‚Üî {corr_matrix.columns[j]}: {corr_val:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Selection Application\n",
    "\n",
    "### Using Correlation for ML\n",
    "High correlation with target ‚Üí potentially useful feature\n",
    "High correlation between features ‚Üí multicollinearity problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load diabetes dataset\n",
    "diabetes = load_diabetes(as_frame=True)\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "# Correlation with target\n",
    "target_corr = X.corrwith(y).abs().sort_values(ascending=False)\n",
    "print(\"üìà Features ranked by correlation with target:\")\n",
    "print(target_corr)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "target_corr.plot(kind='barh', color='steelblue')\n",
    "plt.xlabel('|Correlation| with Target', fontsize=12)\n",
    "plt.title('Feature Importance by Correlation', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Top 3 features: {', '.join(target_corr.head(3).index.tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multicollinearity Detection\n",
    "\n",
    "### Problem\n",
    "When predictors are highly correlated:\n",
    "- Unstable coefficient estimates\n",
    "- Hard to interpret individual effects\n",
    "- Inflated standard errors\n",
    "\n",
    "### Solution\n",
    "Detect pairs with |œÅ| > 0.8 and consider removing one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_multicollinearity(df, threshold=0.8):\n",
    "    \"\"\"Find highly correlated feature pairs\"\"\"\n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper_tri = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    \n",
    "    high_corr = [(column, row, upper_tri.loc[row, column])\n",
    "                 for column in upper_tri.columns\n",
    "                 for row in upper_tri.index\n",
    "                 if upper_tri.loc[row, column] > threshold]\n",
    "    \n",
    "    return high_corr\n",
    "\n",
    "# Test on diabetes dataset\n",
    "high_corr_pairs = detect_multicollinearity(X, threshold=0.5)\n",
    "\n",
    "print(\"‚ö†Ô∏è Multicollinearity Warning (|œÅ| > 0.5):\")\n",
    "if high_corr_pairs:\n",
    "    for feat1, feat2, corr_val in high_corr_pairs:\n",
    "        print(f\"  {feat1} <-> {feat2}: {corr_val:.3f}\")\n",
    "else:\n",
    "    print(\"  None found (good!)\")\n",
    "\n",
    "# Visualize feature correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(X.corr(), annot=True, fmt='.2f', cmap='RdYlGn',\n",
    "            center=0, vmin=-1, vmax=1, square=True)\n",
    "plt.title('Feature Correlation Matrix - Diabetes Dataset', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practice Problems\n",
    "\n",
    "### Problem 1: Compute Correlation\n",
    "Given Cov(X,Y) = 10, Var(X) = 25, Var(Y) = 16, find œÅ(X,Y)\n",
    "\n",
    "### Problem 2: Independence Test\n",
    "If X and Y are independent, prove Cov(X+Y, X-Y) = Var(X) - Var(Y)\n",
    "\n",
    "### Problem 3: Real Data\n",
    "Analyze correlation in the dataset below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SOLUTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Problem 1\n",
    "print(\"\\nüìù Problem 1: Correlation\")\n",
    "cov_xy = 10\n",
    "var_x = 25\n",
    "var_y = 16\n",
    "rho = cov_xy / (np.sqrt(var_x) * np.sqrt(var_y))\n",
    "print(f\"œÅ = Cov(X,Y) / (œÉ_X √ó œÉ_Y)\")\n",
    "print(f\"œÅ = {cov_xy} / (‚àö{var_x} √ó ‚àö{var_y})\")\n",
    "print(f\"œÅ = {cov_xy} / ({np.sqrt(var_x):.0f} √ó {np.sqrt(var_y):.0f})\")\n",
    "print(f\"œÅ = {rho:.2f}\")\n",
    "\n",
    "# Problem 2\n",
    "print(\"\\nüìù Problem 2: Proof\")\n",
    "print(\"Cov(X+Y, X-Y) = E[(X+Y)(X-Y)] - E[X+Y]E[X-Y]\")\n",
    "print(\"              = E[X¬≤ - Y¬≤] - (E[X]+E[Y])(E[X]-E[Y])\")\n",
    "print(\"              = E[X¬≤] - E[Y¬≤] - E[X]¬≤ + E[Y]¬≤\")\n",
    "print(\"              = (E[X¬≤] - E[X]¬≤) - (E[Y¬≤] - E[Y]¬≤)\")\n",
    "print(\"              = Var(X) - Var(Y) ‚úì\")\n",
    "\n",
    "# Problem 3\n",
    "print(\"\\nüìù Problem 3: Dataset Analysis\")\n",
    "data = pd.DataFrame({\n",
    "    'temperature': [20, 22, 25, 27, 30],\n",
    "    'ice_cream_sales': [50, 60, 75, 85, 100],\n",
    "    'crime_rate': [10, 12, 15, 17, 20]\n",
    "})\n",
    "\n",
    "corr = data.corr()\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(corr.round(3))\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Warning: temperature ‚Üî crime_rate correlation = {:.3f}\".format(\n",
    "    corr.loc['temperature', 'crime_rate']\n",
    "))\n",
    "print(\"This is SPURIOUS correlation (both caused by temperature)!\")\n",
    "print(\"Correlation ‚â† Causation\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary & Key Takeaways\n",
    "\n",
    "### üìö Core Concepts\n",
    "1. **Covariance**: $\\text{Cov}(X,Y) = E[XY] - E[X]E[Y]$ - measures joint variability\n",
    "2. **Correlation**: $\\rho = \\text{Cov}(X,Y)/(\\sigma_X\\sigma_Y)$ - normalized, scale-free (-1 to 1)\n",
    "3. **Independence**: $P(X,Y) = P(X)P(Y)$ - strongest relationship (or lack thereof)\n",
    "\n",
    "### üîë Key Relationships\n",
    "```\n",
    "Independence ‚áí Zero Covariance ‚áí Linear Independence\n",
    "     ‚úì              ‚úó                    ‚úó\n",
    "(reverse implications don't hold!)\n",
    "```\n",
    "\n",
    "### ‚ö†Ô∏è Common Pitfalls\n",
    "1. **Y = X¬≤**: Zero correlation but fully dependent\n",
    "2. **Spurious correlation**: Both variables caused by third factor\n",
    "3. **Correlation ‚â† Causation**: Always investigate mechanism\n",
    "4. **Outliers**: Can heavily influence correlation\n",
    "\n",
    "### üéØ Data Science Applications\n",
    "- **Feature Selection**: Select high target correlation\n",
    "- **Multicollinearity**: Remove high inter-feature correlation (|œÅ| > 0.8)\n",
    "- **PCA**: Uses covariance matrix for dimensionality reduction\n",
    "- **Portfolio Theory**: Diversification uses negative correlation\n",
    "\n",
    "### üìñ Important Formulas\n",
    "- $\\text{Var}(aX + bY) = a^2\\text{Var}(X) + b^2\\text{Var}(Y) + 2ab\\text{Cov}(X,Y)$\n",
    "- $\\text{Cov}(X, Y) = 0$ if X, Y independent\n",
    "- $|\\rho_{X,Y}| = 1 \\iff Y = aX + b$ for some a, b\n",
    "\n",
    "### üöÄ Next Week\n",
    "**Week 3: Expectations and Variance of Functions**\n",
    "- Law of Total Expectation\n",
    "- Variance decomposition\n",
    "- Moment generating functions\n",
    "\n",
    "---\n",
    "**üéì End of Week 2**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
