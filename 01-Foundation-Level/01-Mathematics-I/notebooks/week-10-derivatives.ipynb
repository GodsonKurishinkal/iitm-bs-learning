{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10: Derivatives\n",
    "\n",
    "**Course:** Mathematics for Data Science I (BSMA1001)  \n",
    "**Week:** 10 of 12\n",
    "\n",
    "## Learning Objectives\n",
    "- Derivative definition\n",
    "- Power rule, product rule, quotient rule\n",
    "- Chain rule\n",
    "- Critical points and extrema\n",
    "- Applications in optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import optimize, integrate\n",
    "import sympy as sp\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sp.init_printing()\n",
    "%matplotlib inline\n",
    "\n",
    "print('âœ“ Libraries loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ 1. Derivative Definition\n",
    "\n",
    "### Introduction to Derivatives\n",
    "\n",
    "The **derivative** measures the instantaneous rate of change of a function. It's one of the most important concepts in calculus and is fundamental to optimization, machine learning, and data science.\n",
    "\n",
    "**Geometric interpretation:** The derivative at a point is the slope of the tangent line to the curve at that point.\n",
    "\n",
    "**Physical interpretation:** If $f(t)$ represents position at time $t$, then $f'(t)$ represents velocity (rate of change of position).\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1 Formal Definition (Limit Definition)\n",
    "\n",
    "The derivative of $f$ at $x = a$ is:\n",
    "\n",
    "$$f'(a) = \\lim_{h \\to 0} \\frac{f(a + h) - f(a)}{h}$$\n",
    "\n",
    "**Alternative forms:**\n",
    "\n",
    "$$f'(x) = \\lim_{\\Delta x \\to 0} \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}$$\n",
    "\n",
    "$$f'(a) = \\lim_{x \\to a} \\frac{f(x) - f(a)}{x - a}$$\n",
    "\n",
    "**Key insight:** The derivative is a limit of average rates of change (slopes of secant lines) as the interval shrinks to zero.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Notation\n",
    "\n",
    "Multiple notations for derivatives:\n",
    "\n",
    "1. **Lagrange notation:** $f'(x)$, $f''(x)$, $f'''(x)$\n",
    "2. **Leibniz notation:** $\\frac{df}{dx}$, $\\frac{d^2f}{dx^2}$\n",
    "3. **Newton notation:** $\\dot{f}$, $\\ddot{f}$ (for time derivatives)\n",
    "4. **Operator notation:** $D_x f$, $D^2_x f$\n",
    "\n",
    "**In this course, we primarily use:** $f'(x)$ and $\\frac{df}{dx}$\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Differentiability and Continuity\n",
    "\n",
    "**Theorem:** If $f$ is differentiable at $x = a$, then $f$ is continuous at $x = a$.\n",
    "\n",
    "**Contrapositive:** If $f$ is not continuous at $a$, then $f$ is not differentiable at $a$.\n",
    "\n",
    "**Important:** The converse is NOT true!\n",
    "- A function can be continuous but not differentiable\n",
    "- **Example:** $f(x) = |x|$ at $x = 0$ (continuous but sharp corner)\n",
    "\n",
    "**Non-differentiable cases:**\n",
    "1. **Discontinuity:** Function has a jump or break\n",
    "2. **Corner/cusp:** Sharp point (e.g., $|x|$ at 0)\n",
    "3. **Vertical tangent:** Slope is infinite (e.g., $\\sqrt[3]{x}$ at 0)\n",
    "\n",
    "---\n",
    "\n",
    "### 1.4 Computing Derivatives from Definition\n",
    "\n",
    "**Example 1:** Find $f'(x)$ for $f(x) = x^2$\n",
    "\n",
    "$$f'(x) = \\lim_{h \\to 0} \\frac{(x+h)^2 - x^2}{h}$$\n",
    "\n",
    "$$= \\lim_{h \\to 0} \\frac{x^2 + 2xh + h^2 - x^2}{h}$$\n",
    "\n",
    "$$= \\lim_{h \\to 0} \\frac{2xh + h^2}{h} = \\lim_{h \\to 0} (2x + h) = 2x$$\n",
    "\n",
    "**Result:** $(x^2)' = 2x$\n",
    "\n",
    "**Example 2:** Find $f'(x)$ for $f(x) = \\frac{1}{x}$\n",
    "\n",
    "$$f'(x) = \\lim_{h \\to 0} \\frac{\\frac{1}{x+h} - \\frac{1}{x}}{h} = \\lim_{h \\to 0} \\frac{x - (x+h)}{h \\cdot x(x+h)}$$\n",
    "\n",
    "$$= \\lim_{h \\to 0} \\frac{-h}{h \\cdot x(x+h)} = -\\frac{1}{x^2}$$\n",
    "\n",
    "---\n",
    "\n",
    "### 1.5 Data Science Applications\n",
    "\n",
    "**1. Gradient Descent**\n",
    "\n",
    "The derivative tells us the direction of steepest ascent:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha \\cdot \\frac{\\partial L}{\\partial \\theta}$$\n",
    "\n",
    "Where $\\frac{\\partial L}{\\partial \\theta}$ is the gradient (derivative of loss).\n",
    "\n",
    "**2. Backpropagation**\n",
    "\n",
    "Neural networks use chain rule of derivatives to compute gradients.\n",
    "\n",
    "**3. Sensitivity Analysis**\n",
    "\n",
    "How much does output change when input changes? $\\text{Sensitivity} = \\frac{df}{dx}$\n",
    "\n",
    "**4. Marginal Analysis**\n",
    "\n",
    "- **Marginal cost:** $MC = \\frac{dC}{dq}$\n",
    "- **Marginal revenue:** $MR = \\frac{dR}{dq}$\n",
    "\n",
    "**5. Rate of Change**\n",
    "\n",
    "- **Velocity:** $v(t) = \\frac{ds}{dt}$\n",
    "- **Acceleration:** $a(t) = \\frac{dv}{dt} = \\frac{d^2s}{dt^2}$\n",
    "\n",
    "---\n",
    "\n",
    "### 1.6 Common Derivatives (Reference)\n",
    "\n",
    "| Function | Derivative | Notes |\n",
    "|----------|------------|-------|\n",
    "| $x^n$ | $nx^{n-1}$ | Power rule |\n",
    "| $e^x$ | $e^x$ | Exponential |\n",
    "| $\\ln x$ | $\\frac{1}{x}$ | Natural log |\n",
    "| $\\sin x$ | $\\cos x$ | Sine |\n",
    "| $\\cos x$ | $-\\sin x$ | Cosine |\n",
    "| $\\tan x$ | $\\sec^2 x$ | Tangent |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DERIVATIVE DEFINITION - SECTION HEADER\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SECTION 1: DERIVATIVE DEFINITION\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. NUMERICAL DERIVATIVE APPROXIMATION\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. NUMERICAL DERIVATIVE FROM LIMIT DEFINITION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def numerical_derivative(f, x, h=1e-7):\n",
    "    \"\"\"\n",
    "    Compute derivative using limit definition: f'(x) â‰ˆ [f(x+h) - f(x)]/h\n",
    "    \"\"\"\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "def central_difference(f, x, h=1e-5):\n",
    "    \"\"\"\n",
    "    More accurate: f'(x) â‰ˆ [f(x+h) - f(x-h)]/(2h)\n",
    "    \"\"\"\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "# Example: f(x) = xÂ²\n",
    "print(\"\\nExample: f(x) = xÂ² at x = 3\")\n",
    "f_square = lambda x: x**2\n",
    "x_val = 3\n",
    "\n",
    "print(f\"  Analytical: f'(x) = 2x, so f'(3) = 6\")\n",
    "\n",
    "# Test different h values\n",
    "h_values = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "print(f\"\\n  {'h':>10} | {'Forward diff':>15} | {'Error':>12}\")\n",
    "print(\"  \" + \"-\"*42)\n",
    "\n",
    "for h in h_values:\n",
    "    approx = numerical_derivative(f_square, x_val, h)\n",
    "    error = abs(approx - 6)\n",
    "    print(f\"  {h:10.5f} | {approx:15.10f} | {error:12.2e}\")\n",
    "\n",
    "print(f\"\\n  Central difference (h=0.00001): {central_difference(f_square, x_val):.10f}\")\n",
    "print(\"  âœ“ As h â†’ 0, approximation converges to exact derivative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. VISUALIZING SECANT TO TANGENT\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. SECANT LINES APPROACHING TANGENT LINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def visualize_derivative_convergence(f, a, h_values, true_derivative):\n",
    "    \"\"\"\n",
    "    Visualize how secant lines converge to tangent line as h â†’ 0\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(h_values), figsize=(16, 4))\n",
    "    if len(h_values) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    x = np.linspace(a - 2, a + 2, 500)\n",
    "    y = f(x)\n",
    "    \n",
    "    for idx, h in enumerate(h_values):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Plot function\n",
    "        ax.plot(x, y, 'b-', linewidth=2, label='f(x)')\n",
    "        \n",
    "        # Points\n",
    "        fa = f(a)\n",
    "        fah = f(a + h)\n",
    "        ax.plot(a, fa, 'ro', markersize=10, label=f'(a, f(a))', zorder=5)\n",
    "        ax.plot(a + h, fah, 'go', markersize=8, label=f'(a+h, f(a+h))')\n",
    "        \n",
    "        # Secant line\n",
    "        secant_slope = (fah - fa) / h\n",
    "        x_secant = np.array([a - 1, a + h + 1])\n",
    "        y_secant = fa + secant_slope * (x_secant - a)\n",
    "        ax.plot(x_secant, y_secant, 'g--', linewidth=2, \n",
    "                label=f'Secant (m={secant_slope:.3f})')\n",
    "        \n",
    "        # True tangent line\n",
    "        x_tangent = np.array([a - 1, a + 1])\n",
    "        y_tangent = fa + true_derivative * (x_tangent - a)\n",
    "        ax.plot(x_tangent, y_tangent, 'r--', linewidth=2, alpha=0.7,\n",
    "                label=f\"Tangent (m={true_derivative:.3f})\")\n",
    "        \n",
    "        ax.set_xlabel('x', fontsize=11)\n",
    "        ax.set_ylabel('f(x)', fontsize=11)\n",
    "        ax.set_title(f'h = {h:.3f}\\nSlope â‰ˆ {secant_slope:.3f}', \n",
    "                     fontsize=11, fontweight='bold')\n",
    "        ax.legend(fontsize=8, loc='best')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlim([a-1.5, a+1.5])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example: f(x) = xÂ² at x = 2\n",
    "print(\"\\nExample: f(x) = xÂ² at x = 2\")\n",
    "print(\"  f'(2) = 4 (true derivative)\")\n",
    "print(\"\\n  Visualizing convergence as h decreases:\")\n",
    "\n",
    "h_vals = [1.0, 0.5, 0.1, 0.01]\n",
    "visualize_derivative_convergence(lambda x: x**2, 2, h_vals, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. DERIVATIVES FROM FIRST PRINCIPLES - Example 1: xÂ³\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. COMPUTING DERIVATIVES FROM FIRST PRINCIPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Example 1: xÂ³\n",
    "print(\"\\nExample 1: f(x) = xÂ³\")\n",
    "print(\"  Step 1: f'(x) = lim[hâ†’0] [(x+h)Â³ - xÂ³]/h\")\n",
    "print(\"  Step 2: Expand (x+h)Â³ = xÂ³ + 3xÂ²h + 3xhÂ² + hÂ³\")\n",
    "print(\"  Step 3: [(xÂ³ + 3xÂ²h + 3xhÂ² + hÂ³) - xÂ³]/h\")\n",
    "print(\"  Step 4: [3xÂ²h + 3xhÂ² + hÂ³]/h = 3xÂ² + 3xh + hÂ²\")\n",
    "print(\"  Step 5: lim[hâ†’0] (3xÂ² + 3xh + hÂ²) = 3xÂ²\")\n",
    "print(\"  âœ“ Result: (xÂ³)' = 3xÂ²\")\n",
    "\n",
    "# Verify numerically\n",
    "f_cube = lambda x: x**3\n",
    "x_test = 2\n",
    "analytical = 3 * x_test**2\n",
    "numerical = central_difference(f_cube, x_test)\n",
    "print(f\"\\n  Verification at x = {x_test}:\")\n",
    "print(f\"    Analytical: 3({x_test})Â² = {analytical}\")\n",
    "print(f\"    Numerical: {numerical:.10f}\")\n",
    "print(f\"    Error: {abs(analytical - numerical):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. DERIVATIVES FROM FIRST PRINCIPLES - Example 2: âˆšx\n",
    "\"\"\"\n",
    "\n",
    "# Example 2: âˆšx\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example 2: f(x) = âˆšx\")\n",
    "print(\"  Step 1: f'(x) = lim[hâ†’0] [âˆš(x+h) - âˆšx]/h\")\n",
    "print(\"  Step 2: Multiply by conjugate: [âˆš(x+h) - âˆšx][âˆš(x+h) + âˆšx]/[h(âˆš(x+h) + âˆšx)]\")\n",
    "print(\"  Step 3: [(x+h) - x]/[h(âˆš(x+h) + âˆšx)]\")\n",
    "print(\"  Step 4: h/[h(âˆš(x+h) + âˆšx)]\")\n",
    "print(\"  Step 5: 1/(âˆš(x+h) + âˆšx)\")\n",
    "print(\"  Step 6: lim[hâ†’0] 1/(âˆš(x+h) + âˆšx) = 1/(2âˆšx)\")\n",
    "print(\"  âœ“ Result: (âˆšx)' = 1/(2âˆšx)\")\n",
    "\n",
    "# Verify\n",
    "f_sqrt = lambda x: np.sqrt(x)\n",
    "x_test = 4\n",
    "analytical = 1 / (2 * np.sqrt(x_test))\n",
    "numerical = central_difference(f_sqrt, x_test)\n",
    "print(f\"\\n  Verification at x = {x_test}:\")\n",
    "print(f\"    Analytical: 1/(2âˆš{x_test}) = {analytical}\")\n",
    "print(f\"    Numerical: {numerical:.10f}\")\n",
    "print(f\"    Error: {abs(analytical - numerical):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. DERIVATIVES FROM FIRST PRINCIPLES - Example 3: eË£\n",
    "\"\"\"\n",
    "\n",
    "# Example 3: eË£\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example 3: f(x) = eË£\")\n",
    "print(\"  Step 1: f'(x) = lim[hâ†’0] [e^(x+h) - eË£]/h\")\n",
    "print(\"  Step 2: [eË£Â·e^h - eË£]/h = eË£Â·[e^h - 1]/h\")\n",
    "print(\"  Step 3: eË£ Â· lim[hâ†’0] [e^h - 1]/h\")\n",
    "print(\"  Step 4: lim[hâ†’0] [e^h - 1]/h = 1 (special limit)\")\n",
    "print(\"  Step 5: eË£ Â· 1 = eË£\")\n",
    "print(\"  âœ“ Result: (eË£)' = eË£\")\n",
    "\n",
    "# Verify special limit\n",
    "h_vals = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "print(f\"\\n  Verifying lim[hâ†’0] (e^h - 1)/h = 1:\")\n",
    "print(f\"  {'h':>10} | {'(e^h - 1)/h':>15}\")\n",
    "print(\"  \" + \"-\"*28)\n",
    "for h in h_vals:\n",
    "    limit_val = (np.exp(h) - 1) / h\n",
    "    print(f\"  {h:10.5f} | {limit_val:15.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4. DIFFERENTIABILITY VS CONTINUITY - |x| at x=0\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. DIFFERENTIABILITY VS CONTINUITY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Example: |x| at x = 0\n",
    "print(\"\\nExample: f(x) = |x| at x = 0\")\n",
    "print(\"  Is f continuous at 0? YES\")\n",
    "print(\"    lim[xâ†’0] |x| = 0 = f(0) âœ“\")\n",
    "print(\"\\n  Is f differentiable at 0?\")\n",
    "print(\"    Check left and right derivatives:\")\n",
    "\n",
    "h_vals = [0.1, 0.01, 0.001, 0.0001, -0.0001, -0.001, -0.01, -0.1]\n",
    "print(f\"\\n  {'h':>10} | {'[f(0+h) - f(0)]/h':>20} | {'Direction':>10}\")\n",
    "print(\"  \" + \"-\"*45)\n",
    "\n",
    "f_abs = lambda x: np.abs(x)\n",
    "for h in h_vals:\n",
    "    deriv_approx = (f_abs(h) - f_abs(0)) / h\n",
    "    direction = \"Right\" if h > 0 else \"Left\"\n",
    "    print(f\"  {h:10.4f} | {deriv_approx:20.3f} | {direction:>10}\")\n",
    "\n",
    "print(\"\\n  Left derivative: lim[hâ†’0â»] = -1\")\n",
    "print(\"  Right derivative: lim[hâ†’0âº] = +1\")\n",
    "print(\"  Since left â‰  right, |x| is NOT differentiable at 0 âœ—\")\n",
    "print(\"\\n  Key insight: Continuous does NOT imply differentiable!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5. TANGENT LINE COMPUTATION\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. TANGENT LINE EQUATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def compute_tangent_line(f, df, a):\n",
    "    \"\"\"\n",
    "    Compute tangent line at x = a: y = f'(a)(x - a) + f(a)\n",
    "    \"\"\"\n",
    "    fa = f(a)\n",
    "    slope = df(a)\n",
    "    intercept = fa - slope * a\n",
    "    \n",
    "    if intercept >= 0:\n",
    "        eq = f\"y = {slope:.3f}x + {intercept:.3f}\"\n",
    "    else:\n",
    "        eq = f\"y = {slope:.3f}x - {abs(intercept):.3f}\"\n",
    "    \n",
    "    return slope, intercept, eq\n",
    "\n",
    "# Example 1: f(x) = xÂ² at x = 3\n",
    "print(\"\\nExample 1: f(x) = xÂ² at x = 3\")\n",
    "f1 = lambda x: x**2\n",
    "df1 = lambda x: 2*x\n",
    "slope, intercept, eq = compute_tangent_line(f1, df1, 3)\n",
    "print(f\"  f(3) = 9\")\n",
    "print(f\"  f'(3) = 6\")\n",
    "print(f\"  Tangent line: {eq}\")\n",
    "\n",
    "# Example 2: f(x) = xÂ³ - 2xÂ² + 1 at x = 1\n",
    "print(\"\\nExample 2: f(x) = xÂ³ - 2xÂ² + 1 at x = 1\")\n",
    "f2 = lambda x: x**3 - 2*x**2 + 1\n",
    "df2 = lambda x: 3*x**2 - 4*x\n",
    "slope, intercept, eq = compute_tangent_line(f2, df2, 1)\n",
    "print(f\"  f(1) = {f2(1)}\")\n",
    "print(f\"  f'(x) = 3xÂ² - 4x\")\n",
    "print(f\"  f'(1) = {df2(1)}\")\n",
    "print(f\"  Tangent line: {eq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6. COMPREHENSIVE VISUALIZATIONS (9 plots)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"6. COMPREHENSIVE VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.35)\n",
    "\n",
    "# Plot 1: Derivative as slope\n",
    "print(\"\\n  Creating Plot 1: Derivative as slope...\")\n",
    "ax = fig.add_subplot(gs[0, 0])\n",
    "x = np.linspace(-2, 4, 500)\n",
    "f = lambda x: 0.5*x**2 - x + 1\n",
    "df = lambda x: x - 1\n",
    "\n",
    "x_points = [0, 1, 2, 3]\n",
    "colors = ['red', 'green', 'blue', 'purple']\n",
    "\n",
    "ax.plot(x, f(x), 'b-', linewidth=2.5, label='f(x) = 0.5xÂ² - x + 1')\n",
    "\n",
    "for xp, color in zip(x_points, colors):\n",
    "    ax.plot(xp, f(xp), 'o', color=color, markersize=10, zorder=5)\n",
    "    slope = df(xp)\n",
    "    x_tan = np.array([xp - 1, xp + 1])\n",
    "    y_tan = f(xp) + slope * (x_tan - xp)\n",
    "    ax.plot(x_tan, y_tan, '--', color=color, linewidth=1.5, alpha=0.7,\n",
    "            label=f\"x={xp}, m={slope:.1f}\")\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('f(x)', fontsize=11)\n",
    "ax.set_title('Derivative = Slope of Tangent Line', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=8, loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Function and its derivative\n",
    "print(\"  Creating Plot 2: Function and derivative...\")\n",
    "ax = fig.add_subplot(gs[0, 1])\n",
    "x = np.linspace(-3, 3, 500)\n",
    "f_x = x**3 - 3*x\n",
    "df_x = 3*x**2 - 3\n",
    "\n",
    "ax.plot(x, f_x, 'b-', linewidth=2.5, label=\"f(x) = xÂ³ - 3x\")\n",
    "ax.plot(x, df_x, 'r-', linewidth=2.5, label=\"f'(x) = 3xÂ² - 3\")\n",
    "ax.axhline(0, color='black', linewidth=0.8)\n",
    "ax.axvline(0, color='black', linewidth=0.8)\n",
    "\n",
    "critical_pts = [-1, 1]\n",
    "for cp in critical_pts:\n",
    "    idx = np.argmin(np.abs(x - cp))\n",
    "    ax.plot(cp, f_x[idx], 'go', markersize=10, zorder=5)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('y', fontsize=11)\n",
    "ax.set_title('Function and Derivative\\nGreen dots: f\\'=0 (critical points)', \n",
    "             fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9, loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Convergence of difference quotient\n",
    "print(\"  Creating Plot 3: Convergence to derivative...\")\n",
    "ax = fig.add_subplot(gs[0, 2])\n",
    "f = lambda x: x**2\n",
    "x0 = 2\n",
    "true_deriv = 4\n",
    "\n",
    "h_values = np.logspace(-8, 0, 100)\n",
    "approx_derivs = [(f(x0 + h) - f(x0)) / h for h in h_values]\n",
    "\n",
    "ax.semilogx(h_values, approx_derivs, 'b-', linewidth=2.5, label='Approximation')\n",
    "ax.axhline(true_deriv, color='red', linestyle='--', linewidth=2.5, \n",
    "           label=f'True derivative = {true_deriv}')\n",
    "ax.fill_between(h_values, true_deriv - 0.05, true_deriv + 0.05, \n",
    "                alpha=0.2, color='red')\n",
    "\n",
    "ax.set_xlabel('h', fontsize=11)\n",
    "ax.set_ylabel(\"[f(x+h) - f(x)]/h\", fontsize=11)\n",
    "ax.set_title('Convergence to Derivative\\nf(x)=xÂ² at x=2', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3, which='both')\n",
    "ax.set_ylim([3.5, 4.5])\n",
    "\n",
    "# Plot 4: |x| - continuous but not differentiable\n",
    "print(\"  Creating Plot 4: Continuous but not differentiable...\")\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "x = np.linspace(-2, 2, 500)\n",
    "y = np.abs(x)\n",
    "\n",
    "ax.plot(x, y, 'b-', linewidth=3, label='f(x) = |x|')\n",
    "ax.plot(0, 0, 'ro', markersize=15, label='Corner at (0,0)', zorder=5)\n",
    "\n",
    "x_left = np.array([-2, 0])\n",
    "y_left = -x_left\n",
    "ax.plot(x_left, y_left, 'g--', linewidth=2.5, alpha=0.8, label='Left: slope = -1')\n",
    "\n",
    "x_right = np.array([0, 2])\n",
    "y_right = x_right\n",
    "ax.plot(x_right, y_right, 'r--', linewidth=2.5, alpha=0.8, label='Right: slope = +1')\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('f(x)', fontsize=11)\n",
    "ax.set_title('Continuous but NOT Differentiable\\nCorner at x=0', \n",
    "             fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Velocity as derivative\n",
    "print(\"  Creating Plot 5: Velocity as derivative...\")\n",
    "ax = fig.add_subplot(gs[1, 1])\n",
    "t = np.linspace(0, 10, 500)\n",
    "s = lambda t: -0.5*t**2 + 5*t\n",
    "v = lambda t: -t + 5\n",
    "\n",
    "ax.plot(t, s(t), 'b-', linewidth=2.5, label='s(t) = -0.5tÂ² + 5t (position)')\n",
    "ax.plot(t, v(t), 'r-', linewidth=2.5, label=\"v(t) = s'(t) = -t + 5 (velocity)\")\n",
    "ax.axhline(0, color='black', linewidth=0.8)\n",
    "\n",
    "t_stop = 5\n",
    "ax.plot(t_stop, s(t_stop), 'go', markersize=15, label=f'v=0 at t={t_stop}s', zorder=5)\n",
    "ax.axvline(t_stop, color='green', linestyle=':', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('Time t (seconds)', fontsize=11)\n",
    "ax.set_ylabel('Value', fontsize=11)\n",
    "ax.set_title('Velocity as Derivative\\nMaximum position when velocity = 0', \n",
    "             fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9, loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Numerical vs analytical derivative\n",
    "print(\"  Creating Plot 6: Numerical vs analytical...\")\n",
    "ax = fig.add_subplot(gs[1, 2])\n",
    "x = np.linspace(0.1, 5, 200)\n",
    "f = lambda x: np.sin(x)\n",
    "df_true = lambda x: np.cos(x)\n",
    "\n",
    "y_analytical = df_true(x)\n",
    "h = 0.01\n",
    "y_numerical = np.array([(f(xi + h) - f(xi)) / h for xi in x])\n",
    "error = np.abs(y_analytical - y_numerical)\n",
    "\n",
    "ax.plot(x, y_analytical, 'b-', linewidth=2.5, label='Analytical: cos(x)')\n",
    "ax.plot(x, y_numerical, 'r--', linewidth=2, alpha=0.7, label=f'Numerical (h={h})')\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel(\"f'(x)\", fontsize=11)\n",
    "ax.set_title(f'Numerical vs Analytical\\nf(x)=sin(x), max error={error.max():.2e}', \n",
    "             fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 7: Second derivative and concavity\n",
    "print(\"  Creating Plot 7: Second derivative and concavity...\")\n",
    "ax = fig.add_subplot(gs[2, 0])\n",
    "x = np.linspace(-2, 2, 500)\n",
    "f = lambda x: x**4 - 2*x**2\n",
    "ddf = lambda x: 12*x**2 - 4\n",
    "\n",
    "ax.plot(x, f(x), 'b-', linewidth=2.5, label=\"f(x) = xâ´ - 2xÂ²\")\n",
    "ax.plot(x, ddf(x), 'r-', linewidth=2.5, label=\"f''(x) = 12xÂ² - 4\")\n",
    "ax.axhline(0, color='black', linewidth=0.8)\n",
    "\n",
    "inflection = [-np.sqrt(1/3), np.sqrt(1/3)]\n",
    "for ip in inflection:\n",
    "    idx = np.argmin(np.abs(x - ip))\n",
    "    ax.plot(ip, f(x)[idx], 'go', markersize=12, zorder=5)\n",
    "\n",
    "ax.fill_between(x, -5, 5, where=(ddf(x) > 0), alpha=0.15, color='yellow',\n",
    "                label='Concave up (f\">0)')\n",
    "ax.fill_between(x, -5, 5, where=(ddf(x) < 0), alpha=0.15, color='cyan',\n",
    "                label='Concave down (f\"<0)')\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('y', fontsize=11)\n",
    "ax.set_title('Second Derivative & Concavity\\nGreen: inflection points', \n",
    "             fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=8, loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([-4, 4])\n",
    "\n",
    "# Plot 8: Gradient descent visualization\n",
    "print(\"  Creating Plot 8: Gradient descent...\")\n",
    "ax = fig.add_subplot(gs[2, 1])\n",
    "f = lambda x: (x - 3)**2 + 1\n",
    "df = lambda x: 2*(x - 3)\n",
    "\n",
    "x = np.linspace(0, 6, 500)\n",
    "ax.plot(x, f(x), 'b-', linewidth=2.5, label='Loss: L(Î¸) = (Î¸-3)Â²+1')\n",
    "\n",
    "x_current = 0.5\n",
    "learning_rate = 0.3\n",
    "steps = []\n",
    "\n",
    "for i in range(10):\n",
    "    steps.append((x_current, f(x_current)))\n",
    "    gradient = df(x_current)\n",
    "    x_current = x_current - learning_rate * gradient\n",
    "\n",
    "x_path = [s[0] for s in steps]\n",
    "y_path = [s[1] for s in steps]\n",
    "ax.plot(x_path, y_path, 'ro-', markersize=8, linewidth=2, label='GD path', zorder=4)\n",
    "\n",
    "for i in range(len(steps)-1):\n",
    "    ax.annotate('', xy=(x_path[i+1], y_path[i+1]), \n",
    "                xytext=(x_path[i], y_path[i]),\n",
    "                arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "\n",
    "ax.plot(3, 1, 'g*', markersize=25, label='Minimum', zorder=5)\n",
    "ax.plot(x_path[0], y_path[0], 'ko', markersize=10, label='Start', zorder=5)\n",
    "\n",
    "ax.set_xlabel('Parameter Î¸', fontsize=11)\n",
    "ax.set_ylabel('Loss L(Î¸)', fontsize=11)\n",
    "ax.set_title('Gradient Descent\\nÎ¸ â† Î¸ - Î±Â·dL/dÎ¸ (Î±=0.3)', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 9: Rate of change application (population growth)\n",
    "print(\"  Creating Plot 9: Population growth rate...\")\n",
    "ax = fig.add_subplot(gs[2, 2])\n",
    "t = np.linspace(0, 5, 500)\n",
    "P = lambda t: 100 * np.exp(0.3 * t)\n",
    "dP = lambda t: 30 * np.exp(0.3 * t)\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax.plot(t, P(t), color=color, linewidth=2.5, label='P(t) = 100e^(0.3t)')\n",
    "ax.set_xlabel('Time (years)', fontsize=11)\n",
    "ax.set_ylabel('Population', fontsize=11, color=color)\n",
    "ax.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax_twin = ax.twinx()\n",
    "color = 'tab:red'\n",
    "ax_twin.plot(t, dP(t), color=color, linewidth=2.5, label=\"P'(t) = 30e^(0.3t)\")\n",
    "ax_twin.set_ylabel('Growth Rate (per year)', fontsize=11, color=color)\n",
    "ax_twin.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax.set_title('Population Growth\\nDerivative = Growth Rate', \n",
    "             fontsize=11, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "lines1, labels1 = ax.get_legend_handles_labels()\n",
    "lines2, labels2 = ax_twin.get_legend_handles_labels()\n",
    "ax.legend(lines1 + lines2, labels1 + labels2, fontsize=9, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ All 9 visualizations complete\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 1 COMPLETE: Derivative Definition\")\n",
    "print(\"=\"*80)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ 2. Differentiation Rules: Power, Product, Quotient\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Computing derivatives from first principles is tedious. **Differentiation rules** provide shortcuts to find derivatives quickly and efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1 Power Rule\n",
    "\n",
    "**Theorem:** For any real number $n$:\n",
    "\n",
    "$$\\frac{d}{dx}[x^n] = nx^{n-1}$$\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "- $(x^5)' = 5x^4$\n",
    "- $(x^{-2})' = -2x^{-3} = -\\frac{2}{x^3}$\n",
    "- $(\\sqrt{x})' = (x^{1/2})' = \\frac{1}{2}x^{-1/2} = \\frac{1}{2\\sqrt{x}}$\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Product Rule\n",
    "\n",
    "**Theorem:** If $u$ and $v$ are differentiable:\n",
    "\n",
    "$$(uv)' = u'v + uv'$$\n",
    "\n",
    "**Example:** $f(x) = x^2 \\sin x$\n",
    "\n",
    "Let $u = x^2$, $v = \\sin x$\n",
    "- $u' = 2x$\n",
    "- $v' = \\cos x$\n",
    "\n",
    "$$f'(x) = 2x \\cdot \\sin x + x^2 \\cdot \\cos x$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 Quotient Rule\n",
    "\n",
    "**Theorem:** If $u$ and $v$ are differentiable and $v \\neq 0$:\n",
    "\n",
    "$$\\left(\\frac{u}{v}\\right)' = \\frac{u'v - uv'}{v^2}$$\n",
    "\n",
    "**Mnemonic:** \"Low dee-high minus high dee-low, over low-low\"\n",
    "\n",
    "**Example:** $f(x) = \\frac{x^2}{x+1}$\n",
    "\n",
    "$$f'(x) = \\frac{2x(x+1) - x^2 \\cdot 1}{(x+1)^2} = \\frac{x^2 + 2x}{(x+1)^2}$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4 Common Derivatives\n",
    "\n",
    "| Function | Derivative | Rule Used |\n",
    "|----------|------------|-----------|\n",
    "| $c$ | $0$ | Constant |\n",
    "| $x^n$ | $nx^{n-1}$ | Power |\n",
    "| $cf(x)$ | $cf'(x)$ | Constant multiple |\n",
    "| $f \\pm g$ | $f' \\pm g'$ | Sum/Difference |\n",
    "| $fg$ | $f'g + fg'$ | Product |\n",
    "| $\\frac{f}{g}$ | $\\frac{f'g - fg'}{g^2}$ | Quotient |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DIFFERENTIATION RULES - SECTION HEADER\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SECTION 2: POWER, PRODUCT, QUOTIENT RULES\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. POWER RULE DEMONSTRATION\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. POWER RULE: d/dx[x^n] = nx^(n-1)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Symbolic differentiation with SymPy\n",
    "x = sp.Symbol('x')\n",
    "\n",
    "powers = [2, 3, 5, -1, -2, sp.Rational(1,2), sp.Rational(1,3)]\n",
    "\n",
    "print(\"\\nPower Rule Examples:\")\n",
    "print(f\"  {'Function':>15} | {'Derivative':>20}\")\n",
    "print(\"  \" + \"-\"*38)\n",
    "\n",
    "for n in powers:\n",
    "    f = x**n\n",
    "    df = sp.diff(f, x)\n",
    "    f_str = f\"x^{n}\" if isinstance(n, int) else f\"x^({n})\"\n",
    "    df_str = sp.latex(df)\n",
    "    print(f\"  {f_str:>15} | {df_str:>20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. POWER RULE - Numerical Verification\n",
    "\"\"\"\n",
    "\n",
    "# Numerical verification\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Numerical Verification at x = 2:\")\n",
    "\n",
    "x_val = 2\n",
    "h = 1e-7\n",
    "\n",
    "print(f\"\\n  {'Function':>12} | {'Symbolic':>12} | {'Numerical':>12} | {'Error':>10}\")\n",
    "print(\"  \" + \"-\"*52)\n",
    "\n",
    "test_powers = [2, 3, -1, 0.5]\n",
    "for n in test_powers:\n",
    "    f_sym = lambda x: x**n\n",
    "    df_symbolic = n * (x_val**(n-1))\n",
    "    df_numerical = (f_sym(x_val + h) - f_sym(x_val)) / h\n",
    "    error = abs(df_symbolic - df_numerical)\n",
    "    \n",
    "    print(f\"  x^{n:>9} | {df_symbolic:12.6f} | {df_numerical:12.6f} | {error:10.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. PRODUCT RULE DEMONSTRATION\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. PRODUCT RULE: (uv)' = u'v + uv'\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Example 1: xÂ² sin(x)\n",
    "print(\"\\nExample 1: f(x) = xÂ² sin(x)\")\n",
    "\n",
    "x = sp.Symbol('x')\n",
    "u = x**2\n",
    "v = sp.sin(x)\n",
    "f = u * v\n",
    "\n",
    "u_prime = sp.diff(u, x)\n",
    "v_prime = sp.diff(v, x)\n",
    "f_prime_formula = u_prime * v + u * v_prime\n",
    "f_prime_direct = sp.diff(f, x)\n",
    "\n",
    "print(f\"  u = xÂ², u' = {u_prime}\")\n",
    "print(f\"  v = sin(x), v' = {v_prime}\")\n",
    "print(f\"  Product rule: f' = u'v + uv'\")\n",
    "print(f\"             = ({u_prime})(sin(x)) + (xÂ²)({v_prime})\")\n",
    "print(f\"             = {sp.simplify(f_prime_formula)}\")\n",
    "print(f\"  Match: {sp.simplify(f_prime_formula - f_prime_direct) == 0} âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. PRODUCT RULE - Numerical Verification\n",
    "\"\"\"\n",
    "\n",
    "# Numerical verification\n",
    "x_val = 2.0\n",
    "h = 1e-7\n",
    "\n",
    "f_func = lambda x: np.exp(x) * np.log(x)\n",
    "df_numerical = (f_func(x_val + h) - f_func(x_val)) / h\n",
    "df_analytical = np.exp(x_val) * (np.log(x_val) + 1/x_val)\n",
    "\n",
    "print(f\"\\nExample 2: f(x) = e^x ln(x) at x = {x_val}\")\n",
    "print(f\"  Analytical: {df_analytical:.10f}\")\n",
    "print(f\"  Numerical:  {df_numerical:.10f}\")\n",
    "print(f\"  Error:      {abs(df_analytical - df_numerical):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. QUOTIENT RULE DEMONSTRATION\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. QUOTIENT RULE: (u/v)' = (u'v - uv')/vÂ²\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Example: xÂ²/(x+1)\n",
    "print(\"\\nExample: f(x) = xÂ²/(x+1)\")\n",
    "\n",
    "x = sp.Symbol('x')\n",
    "u = x**2\n",
    "v = x + 1\n",
    "f = u / v\n",
    "\n",
    "u_prime = sp.diff(u, x)\n",
    "v_prime = sp.diff(v, x)\n",
    "f_prime_formula = (u_prime * v - u * v_prime) / v**2\n",
    "f_prime_direct = sp.diff(f, x)\n",
    "\n",
    "print(f\"  u = xÂ², u' = {u_prime}\")\n",
    "print(f\"  v = x+1, v' = {v_prime}\")\n",
    "print(f\"  Quotient rule: f' = (u'v - uv')/vÂ²\")\n",
    "print(f\"                   = (2x(x+1) - xÂ²Â·1)/(x+1)Â²\")\n",
    "print(f\"                   = (xÂ² + 2x)/(x+1)Â²\")\n",
    "print(f\"  Simplified: {sp.simplify(f_prime_direct)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. QUOTIENT RULE - Numerical Verification\n",
    "\"\"\"\n",
    "\n",
    "# Numerical verification\n",
    "x_val = 1.5\n",
    "h = 1e-7\n",
    "\n",
    "f_func = lambda x: np.sin(x) / x\n",
    "df_numerical = (f_func(x_val + h) - f_func(x_val)) / h\n",
    "df_analytical = (x_val * np.cos(x_val) - np.sin(x_val)) / x_val**2\n",
    "\n",
    "print(f\"\\nExample: f(x) = sin(x)/x at x = {x_val}\")\n",
    "print(f\"  Analytical: {df_analytical:.10f}\")\n",
    "print(f\"  Numerical:  {df_numerical:.10f}\")\n",
    "print(f\"  Error:      {abs(df_analytical - df_numerical):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4. COMPREHENSIVE VISUALIZATIONS (6 plots)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. COMPREHENSIVE VISUALIZATIONS (6 plots)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig = plt.figure(figsize=(18, 8))\n",
    "gs = fig.add_gridspec(2, 3, hspace=0.35, wspace=0.35)\n",
    "\n",
    "# Plot 1: Power rule\n",
    "ax = fig.add_subplot(gs[0, 0])\n",
    "x_vals = np.linspace(0.1, 3, 500)\n",
    "\n",
    "powers_to_plot = [1, 2, 3, 0.5]\n",
    "colors = ['red', 'blue', 'green', 'purple']\n",
    "\n",
    "for n, color in zip(powers_to_plot, colors):\n",
    "    dy = n * x_vals**(n-1)\n",
    "    ax.plot(x_vals, dy, color=color, linewidth=2, label=f\"(x^{n})' = {n}x^{n-1}\")\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel(\"f'(x)\", fontsize=11)\n",
    "ax.set_title('Power Rule', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Product rule\n",
    "ax = fig.add_subplot(gs[0, 1])\n",
    "x_vals = np.linspace(0, 2*np.pi, 500)\n",
    "\n",
    "f = x_vals * np.sin(x_vals)\n",
    "f_prime = np.sin(x_vals) + x_vals * np.cos(x_vals)\n",
    "\n",
    "ax.plot(x_vals, f, 'b-', linewidth=2.5, label='f(x) = xÂ·sin(x)')\n",
    "ax.plot(x_vals, f_prime, 'r-', linewidth=2.5, label=\"f'(x) = sin(x) + xÂ·cos(x)\")\n",
    "ax.axhline(0, color='black', linewidth=0.8)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('y', fontsize=11)\n",
    "ax.set_title('Product Rule', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Quotient rule\n",
    "ax = fig.add_subplot(gs[0, 2])\n",
    "x_vals = np.linspace(0.1, 5, 500)\n",
    "\n",
    "f = x_vals**2 / (x_vals + 1)\n",
    "df = (2*x_vals*(x_vals + 1) - x_vals**2) / (x_vals + 1)**2\n",
    "\n",
    "ax.plot(x_vals, f, 'b-', linewidth=2.5, label='f(x) = xÂ²/(x+1)')\n",
    "ax.plot(x_vals, df, 'r-', linewidth=2.5, label=\"f'(x)\")\n",
    "ax.axhline(0, color='black', linewidth=0.8)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('y', fontsize=11)\n",
    "ax.set_title('Quotient Rule', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Polynomial derivative\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "x_vals = np.linspace(-3, 3, 500)\n",
    "\n",
    "f = lambda x: x**4 - 4*x**3 + 6*x**2\n",
    "df = lambda x: 4*x**3 - 12*x**2 + 12*x\n",
    "\n",
    "ax.plot(x_vals, f(x_vals), 'b-', linewidth=2.5, label='f(x) = xâ´ - 4xÂ³ + 6xÂ²')\n",
    "ax.plot(x_vals, df(x_vals), 'r-', linewidth=2.5, label=\"f'(x) = 4xÂ³ - 12xÂ² + 12x\")\n",
    "ax.axhline(0, color='black', linewidth=0.8)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('y', fontsize=11)\n",
    "ax.set_title('Polynomial Differentiation', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: sin(x)/x\n",
    "ax = fig.add_subplot(gs[1, 1])\n",
    "x_vals = np.linspace(-10, 10, 500)\n",
    "x_vals = x_vals[x_vals != 0]\n",
    "\n",
    "f = np.sin(x_vals) / x_vals\n",
    "df = (x_vals * np.cos(x_vals) - np.sin(x_vals)) / x_vals**2\n",
    "\n",
    "ax.plot(x_vals, f, 'b-', linewidth=2.5, label='f(x) = sin(x)/x')\n",
    "ax.plot(x_vals, df, 'r-', linewidth=2.5, label=\"f'(x)\")\n",
    "ax.axhline(0, color='black', linewidth=0.8)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('y', fontsize=11)\n",
    "ax.set_title('Quotient Rule: sinc function', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([-1, 1.5])\n",
    "\n",
    "# Plot 6: Tangent lines comparison\n",
    "ax = fig.add_subplot(gs[1, 2])\n",
    "\n",
    "functions = [\n",
    "    (lambda x: x**2, lambda x: 2*x, 'xÂ²', 'blue'),\n",
    "    (lambda x: x**3, lambda x: 3*x**2, 'xÂ³', 'red'),\n",
    "]\n",
    "\n",
    "x_point = 1\n",
    "x_range = np.linspace(-0.5, 2.5, 500)\n",
    "\n",
    "for f, df, label, color in functions:\n",
    "    ax.plot(x_range, f(x_range), color=color, linewidth=2, label=f'f={label}', alpha=0.7)\n",
    "    \n",
    "    slope = df(x_point)\n",
    "    y_tan = f(x_point) + slope * (x_range - x_point)\n",
    "    ax.plot(x_range, y_tan, '--', color=color, linewidth=1.5, alpha=0.5,\n",
    "            label=f\"m={slope:.1f}\")\n",
    "    \n",
    "    ax.plot(x_point, f(x_point), 'o', color=color, markersize=8)\n",
    "\n",
    "ax.axvline(x_point, color='black', linestyle=':', alpha=0.5)\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('y', fontsize=11)\n",
    "ax.set_title(f'Tangent Lines at x={x_point}', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ All 6 visualizations complete\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 2 COMPLETE: Differentiation Rules\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”— 3. Chain Rule\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The **chain rule** allows us to differentiate **composite functions** - functions within functions.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1 Chain Rule Formula\n",
    "\n",
    "**Theorem:** If $y = f(g(x))$, then:\n",
    "\n",
    "$$\\frac{dy}{dx} = f'(g(x)) \\cdot g'(x)$$\n",
    "\n",
    "**Leibniz notation:**\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$$\n",
    "\n",
    "where $u = g(x)$ and $y = f(u)$.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 Examples\n",
    "\n",
    "**Example 1:** $f(x) = (x^2 + 1)^{10}$\n",
    "\n",
    "Let $u = x^2 + 1$, then $y = u^{10}$\n",
    "\n",
    "$$\\frac{dy}{du} = 10u^9, \\quad \\frac{du}{dx} = 2x$$\n",
    "\n",
    "$$f'(x) = 10(x^2 + 1)^9 \\cdot 2x = 20x(x^2 + 1)^9$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 Applications in ML\n",
    "\n",
    "**Backpropagation** is essentially repeated application of the chain rule!\n",
    "\n",
    "For neural network: $y = f_3(f_2(f_1(x)))$\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{dy}{df_2} \\cdot \\frac{df_2}{df_1} \\cdot \\frac{df_1}{dx}$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. CHAIN RULE EXAMPLES\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. CHAIN RULE EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Symbolic differentiation\n",
    "x = sp.Symbol('x')\n",
    "\n",
    "# Example 1: (xÂ² + 1)^10\n",
    "print(\"\\nExample 1: f(x) = (xÂ² + 1)^10\")\n",
    "f1 = (x**2 + 1)**10\n",
    "df1 = sp.diff(f1, x)\n",
    "print(f\"  Inner function: u = xÂ² + 1\")\n",
    "print(f\"  Outer function: y = u^10\")\n",
    "print(f\"  du/dx = 2x\")\n",
    "print(f\"  dy/du = 10u^9\")\n",
    "print(f\"  f'(x) = 10(xÂ² + 1)^9 Â· 2x = 20x(xÂ² + 1)^9\")\n",
    "print(f\"  SymPy result: {sp.simplify(df1)}\")\n",
    "\n",
    "# Example 2: sin(3xÂ²)\n",
    "print(\"\\nExample 2: f(x) = sin(3xÂ²)\")\n",
    "f2 = sp.sin(3*x**2)\n",
    "df2 = sp.diff(f2, x)\n",
    "print(f\"  Inner function: u = 3xÂ²\")\n",
    "print(f\"  Outer function: y = sin(u)\")\n",
    "print(f\"  du/dx = 6x\")\n",
    "print(f\"  dy/du = cos(u)\")\n",
    "print(f\"  f'(x) = cos(3xÂ²) Â· 6x = 6xÂ·cos(3xÂ²)\")\n",
    "print(f\"  SymPy result: {df2}\")\n",
    "\n",
    "# Example 3: e^(xÂ³)\n",
    "print(\"\\nExample 3: f(x) = e^(xÂ³)\")\n",
    "f3 = sp.exp(x**3)\n",
    "df3 = sp.diff(f3, x)\n",
    "print(f\"  Inner function: u = xÂ³\")\n",
    "print(f\"  Outer function: y = e^u\")\n",
    "print(f\"  du/dx = 3xÂ²\")\n",
    "print(f\"  dy/du = e^u\")\n",
    "print(f\"  f'(x) = e^(xÂ³) Â· 3xÂ² = 3xÂ²e^(xÂ³)\")\n",
    "print(f\"  SymPy result: {df3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. CHAIN RULE - Numerical Verification\n",
    "\"\"\"\n",
    "\n",
    "# Numerical verification\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Numerical Verification at x = 2:\")\n",
    "\n",
    "x_val = 2.0\n",
    "h = 1e-7\n",
    "\n",
    "funcs = [\n",
    "    (lambda x: (x**2 + 1)**10, lambda x: 20*x*(x**2 + 1)**9, \"(xÂ²+1)^10\"),\n",
    "    (lambda x: np.sin(3*x**2), lambda x: 6*x*np.cos(3*x**2), \"sin(3xÂ²)\"),\n",
    "    (lambda x: np.exp(x**3), lambda x: 3*x**2*np.exp(x**3), \"e^(xÂ³)\")\n",
    "]\n",
    "\n",
    "print(f\"\\n  {'Function':>12} | {'Analytical':>12} | {'Numerical':>12} | {'Error':>10}\")\n",
    "print(\"  \" + \"-\"*52)\n",
    "\n",
    "for f, df_analytical, name in funcs:\n",
    "    df_numerical = (f(x_val + h) - f(x_val)) / h\n",
    "    df_exact = df_analytical(x_val)\n",
    "    error = abs(df_numerical - df_exact)\n",
    "    print(f\"  {name:>12} | {df_exact:12.4f} | {df_numerical:12.4f} | {error:10.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. BACKPROPAGATION SIMULATION\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. BACKPROPAGATION (CHAIN RULE IN NEURAL NETWORKS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nSimple 2-layer network: Loss = (ReLU(wÂ·x + b))Â²\")\n",
    "print(\"  Layer 1: zâ‚ = wÂ·x + b\")\n",
    "print(\"  Activation: aâ‚ = max(0, zâ‚)\")\n",
    "print(\"  Loss: L = aâ‚Â²\")\n",
    "\n",
    "# Forward pass\n",
    "x_input = 2.0\n",
    "w = 1.5\n",
    "b = 0.5\n",
    "\n",
    "z1 = w * x_input + b\n",
    "a1 = max(0, z1)\n",
    "loss = a1**2\n",
    "\n",
    "print(f\"\\nForward pass:\")\n",
    "print(f\"  Input: x = {x_input}\")\n",
    "print(f\"  zâ‚ = {w}Â·{x_input} + {b} = {z1}\")\n",
    "print(f\"  aâ‚ = ReLU({z1}) = {a1}\")\n",
    "print(f\"  Loss = {a1}Â² = {loss}\")\n",
    "\n",
    "# Backward pass (chain rule)\n",
    "print(f\"\\nBackward pass (computing dL/dw using chain rule):\")\n",
    "\n",
    "dL_da1 = 2 * a1\n",
    "print(f\"  âˆ‚L/âˆ‚aâ‚ = 2aâ‚ = {dL_da1:.4f}\")\n",
    "\n",
    "da1_dz1 = 1 if z1 > 0 else 0\n",
    "print(f\"  âˆ‚aâ‚/âˆ‚zâ‚ = {da1_dz1} (ReLU derivative: 1 if z>0, else 0)\")\n",
    "\n",
    "dz1_dw = x_input\n",
    "print(f\"  âˆ‚zâ‚/âˆ‚w = x = {dz1_dw}\")\n",
    "\n",
    "dL_dw = dL_da1 * da1_dz1 * dz1_dw\n",
    "print(f\"\\n  Chain rule: âˆ‚L/âˆ‚w = (âˆ‚L/âˆ‚aâ‚) Â· (âˆ‚aâ‚/âˆ‚zâ‚) Â· (âˆ‚zâ‚/âˆ‚w)\")\n",
    "print(f\"            = {dL_da1:.4f} Â· {da1_dz1} Â· {dz1_dw}\")\n",
    "print(f\"            = {dL_dw:.4f}\")\n",
    "print(\"  âœ“ Gradient computed! Can now update: w_new = w - Î±Â·âˆ‚L/âˆ‚w\")\n",
    "\n",
    "# Gradient descent update\n",
    "alpha = 0.1\n",
    "w_new = w - alpha * dL_dw\n",
    "print(f\"\\n  With learning rate Î± = {alpha}:\")\n",
    "print(f\"  w_new = {w} - {alpha}Â·{dL_dw:.4f} = {w_new:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. NESTED CHAIN RULE\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. NESTED CHAIN RULE (MULTIPLE COMPOSITIONS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Example: sin(e^(xÂ²))\n",
    "print(\"\\nExample: f(x) = sin(e^(xÂ²))\")\n",
    "print(\"  Let u = xÂ², v = e^u, y = sin(v)\")\n",
    "print(\"  âˆ‚u/âˆ‚x = 2x\")\n",
    "print(\"  âˆ‚v/âˆ‚u = e^u = e^(xÂ²)\")\n",
    "print(\"  âˆ‚y/âˆ‚v = cos(v) = cos(e^(xÂ²))\")\n",
    "print(\"\\n  Chain rule: dy/dx = (dy/dv)Â·(dv/du)Â·(du/dx)\")\n",
    "print(\"            = cos(e^(xÂ²))Â·e^(xÂ²)Â·2x\")\n",
    "print(\"            = 2xÂ·e^(xÂ²)Â·cos(e^(xÂ²))\")\n",
    "\n",
    "# Verify with SymPy\n",
    "x = sp.Symbol('x')\n",
    "f_nested = sp.sin(sp.exp(x**2))\n",
    "df_nested = sp.diff(f_nested, x)\n",
    "print(f\"\\n  SymPy verification: {df_nested}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4. COMPREHENSIVE VISUALIZATIONS (6 plots)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. COMPREHENSIVE VISUALIZATIONS (6 plots)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.35, wspace=0.35)\n",
    "\n",
    "# Plot 1: (xÂ²+1)Â³ and derivative\n",
    "print(\"\\n  Creating Plot 1: Polynomial composite...\")\n",
    "ax = fig.add_subplot(gs[0, 0])\n",
    "x_vals = np.linspace(-2, 2, 500)\n",
    "\n",
    "f = (x_vals**2 + 1)**3\n",
    "df = 6*x_vals*(x_vals**2 + 1)**2\n",
    "\n",
    "ax.plot(x_vals, f, 'b-', linewidth=2.5, label='f(x) = (xÂ²+1)Â³')\n",
    "ax.plot(x_vals, df, 'r-', linewidth=2.5, label=\"f'(x) = 6x(xÂ²+1)Â²\")\n",
    "ax.axhline(0, color='black', linewidth=0.8)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('y', fontsize=11)\n",
    "ax.set_title('Chain Rule: (xÂ²+1)Â³', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: sin(3x) and derivative\n",
    "print(\"  Creating Plot 2: Trigonometric composite...\")\n",
    "ax = fig.add_subplot(gs[0, 1])\n",
    "x_vals = np.linspace(0, 2*np.pi, 500)\n",
    "\n",
    "f = np.sin(3*x_vals)\n",
    "df = 3*np.cos(3*x_vals)\n",
    "\n",
    "ax.plot(x_vals, f, 'b-', linewidth=2.5, label='f(x) = sin(3x)')\n",
    "ax.plot(x_vals, df, 'r-', linewidth=2.5, label=\"f'(x) = 3cos(3x)\")\n",
    "ax.axhline(0, color='black', linewidth=0.8)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('y', fontsize=11)\n",
    "ax.set_title('Chain Rule: sin(3x)', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: e^(xÂ²) and derivative\n",
    "print(\"  Creating Plot 3: Exponential composite...\")\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "x_vals = np.linspace(-2, 2, 500)\n",
    "\n",
    "f = np.exp(x_vals**2)\n",
    "df = 2*x_vals*np.exp(x_vals**2)\n",
    "\n",
    "ax.plot(x_vals, f, 'b-', linewidth=2.5, label='f(x) = e^(xÂ²)')\n",
    "ax.plot(x_vals, df, 'r-', linewidth=2.5, label=\"f'(x) = 2xÂ·e^(xÂ²)\")\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('y', fontsize=11)\n",
    "ax.set_title('Chain Rule: e^(xÂ²)', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([0, 20])\n",
    "\n",
    "# Plot 4: âˆš(xÂ²+1) and derivative\n",
    "print(\"  Creating Plot 4: Square root composite...\")\n",
    "ax = fig.add_subplot(gs[1, 1])\n",
    "x_vals = np.linspace(-3, 3, 500)\n",
    "\n",
    "f = np.sqrt(x_vals**2 + 1)\n",
    "df = x_vals / np.sqrt(x_vals**2 + 1)\n",
    "\n",
    "ax.plot(x_vals, f, 'b-', linewidth=2.5, label='f(x) = âˆš(xÂ²+1)')\n",
    "ax.plot(x_vals, df, 'r-', linewidth=2.5, label=\"f'(x) = x/âˆš(xÂ²+1)\")\n",
    "ax.axhline(0, color='black', linewidth=0.8)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('y', fontsize=11)\n",
    "ax.set_title('Chain Rule: âˆš(xÂ²+1)', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Backpropagation flow diagram\n",
    "print(\"  Creating Plot 5: Backpropagation diagram...\")\n",
    "ax = fig.add_subplot(gs[2, 0])\n",
    "ax.text(0.5, 0.9, 'Neural Network: Backpropagation', \n",
    "        ha='center', fontsize=13, fontweight='bold')\n",
    "\n",
    "ax.text(0.5, 0.75, 'Forward Pass âœ', \n",
    "        ha='center', fontsize=11, color='blue', fontweight='bold')\n",
    "ax.text(0.5, 0.65, 'x â†’ zâ‚=wx+b â†’ aâ‚=ReLU(zâ‚) â†’ L=aâ‚Â²', \n",
    "        ha='center', fontsize=10, family='monospace')\n",
    "\n",
    "ax.text(0.5, 0.45, 'â¬… Backward Pass (Gradients)', \n",
    "        ha='center', fontsize=11, color='red', fontweight='bold')\n",
    "ax.text(0.5, 0.35, 'âˆ‚L/âˆ‚x â† âˆ‚L/âˆ‚w â† âˆ‚L/âˆ‚aâ‚ â† âˆ‚L/âˆ‚L', \n",
    "        ha='center', fontsize=10, family='monospace', color='red')\n",
    "\n",
    "ax.text(0.5, 0.15, 'Chain Rule: âˆ‚L/âˆ‚w = (âˆ‚L/âˆ‚aâ‚)Â·(âˆ‚aâ‚/âˆ‚zâ‚)Â·(âˆ‚zâ‚/âˆ‚w)', \n",
    "        ha='center', fontsize=10, bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.3))\n",
    "\n",
    "ax.text(0.5, 0.02, 'âœ“ Used to train neural networks!', \n",
    "        ha='center', fontsize=10, style='italic', color='green')\n",
    "\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.axis('off')\n",
    "\n",
    "# Plot 6: Comparison of chain rule applications\n",
    "print(\"  Creating Plot 6: Multiple composites comparison...\")\n",
    "ax = fig.add_subplot(gs[2, 1])\n",
    "x_vals = np.linspace(0.1, 3, 500)\n",
    "\n",
    "functions = [\n",
    "    (lambda x: (x**2)**3, lambda x: 6*x**5, '(xÂ²)Â³', 'blue'),\n",
    "    (lambda x: x**(2**3), lambda x: 8*x**7, 'x^(2Â³)', 'red'),\n",
    "]\n",
    "\n",
    "for f, df, label, color in functions:\n",
    "    ax.plot(x_vals, df(x_vals), color=color, linewidth=2.5, \n",
    "            label=f\"{label}' = {label.replace('(', '').replace(')', '')} derivative\", alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel(\"f'(x)\", fontsize=11)\n",
    "ax.set_title('Chain Rule vs Power Rule\\nNote: (xÂ²)Â³ â‰  x^(2Â³)', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ All 6 visualizations complete\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 3 COMPLETE: Chain Rule\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CHAIN RULE - SECTION HEADER\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SECTION 3: CHAIN RULE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ 4. Critical Points and Extrema\n",
    "\n",
    "### Introduction\n",
    "\n",
    "**Critical points** are where the derivative equals zero or is undefined. They help us find **maximum** and **minimum** values of functions.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1 Definitions\n",
    "\n",
    "**Critical Point:** A point $x = c$ where:\n",
    "- $f'(c) = 0$, OR\n",
    "- $f'(c)$ is undefined\n",
    "\n",
    "**Local Maximum:** $f(c) \\geq f(x)$ for all $x$ near $c$\n",
    "\n",
    "**Local Minimum:** $f(c) \\leq f(x)$ for all $x$ near $c$\n",
    "\n",
    "**Global Maximum:** $f(c) \\geq f(x)$ for all $x$ in domain\n",
    "\n",
    "**Global Minimum:** $f(c) \\leq f(x)$ for all $x$ in domain\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 First Derivative Test\n",
    "\n",
    "To classify critical points using $f'(x)$:\n",
    "\n",
    "| $f'$ changes from | Type of critical point |\n",
    "|-------------------|------------------------|\n",
    "| + to - | Local maximum |\n",
    "| - to + | Local minimum |\n",
    "| No change | Neither (inflection) |\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3 Second Derivative Test\n",
    "\n",
    "At a critical point $x = c$ where $f'(c) = 0$:\n",
    "\n",
    "- If $f''(c) > 0$: **Local minimum** (concave up âŒ£)\n",
    "- If $f''(c) < 0$: **Local maximum** (concave down âŒ¢)\n",
    "- If $f''(c) = 0$: Test is **inconclusive** (use first derivative test)\n",
    "\n",
    "---\n",
    "\n",
    "### 4.4 Example\n",
    "\n",
    "Find extrema of $f(x) = x^3 - 3x^2 - 9x + 5$\n",
    "\n",
    "**Step 1:** Find $f'(x)$\n",
    "\n",
    "$$f'(x) = 3x^2 - 6x - 9$$\n",
    "\n",
    "**Step 2:** Set $f'(x) = 0$\n",
    "\n",
    "$$3x^2 - 6x - 9 = 0$$\n",
    "$$x^2 - 2x - 3 = 0$$\n",
    "$$(x - 3)(x + 1) = 0$$\n",
    "$$x = 3 \\text{ or } x = -1$$\n",
    "\n",
    "**Step 3:** Second derivative test\n",
    "\n",
    "$$f''(x) = 6x - 6$$\n",
    "\n",
    "- At $x = -1$: $f''(-1) = -12 < 0$ â†’ **Local maximum**\n",
    "- At $x = 3$: $f''(3) = 12 > 0$ â†’ **Local minimum**\n",
    "\n",
    "---\n",
    "\n",
    "### 4.5 Applications\n",
    "\n",
    "**1. Profit Maximization**\n",
    "\n",
    "Find production level where $\\frac{d(\\text{Profit})}{dq} = 0$\n",
    "\n",
    "**2. Loss Minimization (ML)**\n",
    "\n",
    "Find parameters where $\\frac{\\partial L}{\\partial \\theta} = 0$\n",
    "\n",
    "**3. Data Analysis**\n",
    "\n",
    "Find peaks and troughs in time series data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CRITICAL POINTS AND EXTREMA - SECTION HEADER\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SECTION 4: CRITICAL POINTS AND EXTREMA\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. FINDING CRITICAL POINTS - Example 1\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. FINDING CRITICAL POINTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Example 1: f(x) = xÂ³ - 3xÂ² - 9x + 5\n",
    "print(\"\\nExample 1: f(x) = xÂ³ - 3xÂ² - 9x + 5\")\n",
    "\n",
    "x = sp.Symbol('x')\n",
    "f = x**3 - 3*x**2 - 9*x + 5\n",
    "df = sp.diff(f, x)\n",
    "ddf = sp.diff(df, x)\n",
    "\n",
    "print(f\"  f'(x) = {df}\")\n",
    "print(f\"  f''(x) = {ddf}\")\n",
    "\n",
    "# Solve f'(x) = 0\n",
    "critical_points = sp.solve(df, x)\n",
    "print(f\"\\n  Critical points (f'=0): {critical_points}\")\n",
    "\n",
    "# Classify using second derivative test\n",
    "print(\"\\n  Classification using second derivative test:\")\n",
    "for cp in critical_points:\n",
    "    cp_val = float(cp)\n",
    "    f_val = float(f.subs(x, cp))\n",
    "    ddf_val = float(ddf.subs(x, cp))\n",
    "    \n",
    "    if ddf_val > 0:\n",
    "        classification = \"Local MINIMUM\"\n",
    "    elif ddf_val < 0:\n",
    "        classification = \"Local MAXIMUM\"\n",
    "    else:\n",
    "        classification = \"Inconclusive\"\n",
    "    \n",
    "    print(f\"    x = {cp_val}: f({cp_val:.0f}) = {f_val:.2f}, f''({cp_val:.0f}) = {ddf_val:.0f} â†’ {classification}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. FIRST DERIVATIVE TEST\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. FIRST DERIVATIVE TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nFor f(x) = xÂ³ - 3xÂ² - 9x + 5 with critical points at x = -1, 3\")\n",
    "\n",
    "# Test intervals around critical points\n",
    "test_points = {\n",
    "    'x < -1': -2,\n",
    "    '-1 < x < 3': 0,\n",
    "    'x > 3': 4\n",
    "}\n",
    "\n",
    "f_func = lambda x: x**3 - 3*x**2 - 9*x + 5\n",
    "df_func = lambda x: 3*x**2 - 6*x - 9\n",
    "\n",
    "print(f\"\\n  {'Interval':>12} | {'Test point':>11} | {'f\\'(x)':>8} | {'Sign':>6}\")\n",
    "print(\"  \" + \"-\"*45)\n",
    "\n",
    "for interval, test_pt in test_points.items():\n",
    "    df_val = df_func(test_pt)\n",
    "    sign = \"+\" if df_val > 0 else \"-\"\n",
    "    print(f\"  {interval:>12} | {test_pt:>11} | {df_val:>8.1f} | {sign:>6}\")\n",
    "\n",
    "print(\"\\n  Analysis:\")\n",
    "print(\"    At x = -1: f' changes from + to - â†’ LOCAL MAXIMUM\")\n",
    "print(\"    At x = 3: f' changes from - to + â†’ LOCAL MINIMUM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4. USING SCIPY FOR OPTIMIZATION\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. USING SCIPY FOR FINDING EXTREMA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find minimum\n",
    "def f_to_minimize(x):\n",
    "    return x**3 - 3*x**2 - 9*x + 5\n",
    "\n",
    "result_min = optimize.minimize_scalar(f_to_minimize, bounds=(-10, 10), method='bounded')\n",
    "print(f\"\\nMinimizing f(x) = xÂ³ - 3xÂ² - 9x + 5:\")\n",
    "print(f\"  Minimum at x = {result_min.x:.4f}\")\n",
    "print(f\"  Minimum value = {result_min.fun:.4f}\")\n",
    "\n",
    "# Find maximum (minimize negative)\n",
    "result_max = optimize.minimize_scalar(lambda x: -f_to_minimize(x), bounds=(-10, 10), method='bounded')\n",
    "print(f\"\\nMaximizing f(x) (minimize -f(x)):\")\n",
    "print(f\"  Maximum at x = {result_max.x:.4f}\")\n",
    "print(f\"  Maximum value = {-result_max.fun:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5. COMPREHENSIVE VISUALIZATIONS (6 plots) - Section 4 Complete\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. COMPREHENSIVE VISUALIZATIONS (6 plots)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Import signal for peak finding\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.35, wspace=0.35)\n",
    "\n",
    "# Plot 1: Critical points and classification\n",
    "print(\"\\n  Creating Plot 1: Critical points...\")\n",
    "ax = fig.add_subplot(gs[0, 0])\n",
    "x_vals = np.linspace(-2, 4, 500)\n",
    "f_vals = x_vals**3 - 3*x_vals**2 - 9*x_vals + 5\n",
    "df_vals = 3*x_vals**2 - 6*x_vals - 9\n",
    "\n",
    "ax.plot(x_vals, f_vals, 'b-', linewidth=2.5, label='f(x)')\n",
    "ax.plot(x_vals, df_vals, 'r--', linewidth=2, label=\"f'(x)\", alpha=0.7)\n",
    "ax.axhline(0, color='black', linewidth=0.8)\n",
    "\n",
    "# Mark critical points\n",
    "critical_x = [-1, 3]\n",
    "for cx in critical_x:\n",
    "    cy = cx**3 - 3*cx**2 - 9*cx + 5\n",
    "    ax.plot(cx, cy, 'go', markersize=12, zorder=5)\n",
    "    ax.annotate(f'({cx}, {cy:.1f})', xy=(cx, cy), xytext=(cx+0.5, cy+3),\n",
    "                fontsize=9, arrowprops=dict(arrowstyle='->', lw=1))\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('y', fontsize=11)\n",
    "ax.set_title('Critical Points\\nGreen dots: f\\'=0', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: First derivative test\n",
    "print(\"  Creating Plot 2: First derivative test...\")\n",
    "ax = fig.add_subplot(gs[0, 1])\n",
    "x_vals = np.linspace(-3, 3, 500)\n",
    "f_vals = x_vals**3 - 3*x_vals\n",
    "df_vals = 3*x_vals**2 - 3\n",
    "\n",
    "ax.plot(x_vals, df_vals, 'r-', linewidth=2.5, label=\"f'(x) = 3xÂ² - 3\")\n",
    "ax.axhline(0, color='black', linewidth=0.8)\n",
    "ax.axvline(-1, color='green', linestyle=':', alpha=0.5, label='Critical points')\n",
    "ax.axvline(1, color='green', linestyle=':', alpha=0.5)\n",
    "\n",
    "# Shade regions\n",
    "ax.fill_between(x_vals, 0, df_vals, where=(df_vals > 0), alpha=0.2, color='green', label='f\\' > 0 (increasing)')\n",
    "ax.fill_between(x_vals, 0, df_vals, where=(df_vals < 0), alpha=0.2, color='red', label='f\\' < 0 (decreasing)')\n",
    "\n",
    "ax.plot([-1, 1], [0, 0], 'go', markersize=12, zorder=5)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel(\"f'(x)\", fontsize=11)\n",
    "ax.set_title('First Derivative Test\\nSign changes indicate extrema', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Second derivative and concavity\n",
    "print(\"  Creating Plot 3: Second derivative test...\")\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "x_vals = np.linspace(-2, 4, 500)\n",
    "f_vals = x_vals**3 - 3*x_vals**2 - 9*x_vals + 5\n",
    "ddf_vals = 6*x_vals - 6\n",
    "\n",
    "ax.plot(x_vals, f_vals, 'b-', linewidth=2.5, label='f(x)')\n",
    "ax.axhline(0, color='black', linewidth=0.8)\n",
    "\n",
    "# Mark critical points with second derivative info\n",
    "ax.plot(-1, f_vals[np.argmin(np.abs(x_vals + 1))], 'ro', markersize=15, \n",
    "        label='Max (f\"<0)', zorder=5)\n",
    "ax.plot(3, f_vals[np.argmin(np.abs(x_vals - 3))], 'go', markersize=15,\n",
    "        label='Min (f\">0)', zorder=5)\n",
    "\n",
    "# Shade concavity\n",
    "ax.fill_between(x_vals, -30, 30, where=(ddf_vals > 0), alpha=0.1, color='yellow',\n",
    "                label='Concave up (f\">0)')\n",
    "ax.fill_between(x_vals, -30, 30, where=(ddf_vals < 0), alpha=0.1, color='cyan',\n",
    "                label='Concave down (f\"<0)')\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('f(x)', fontsize=11)\n",
    "ax.set_title('Second Derivative Test\\nConcavity determines extrema type', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([-30, 30])\n",
    "\n",
    "# Plot 4: Global vs local extrema\n",
    "print(\"  Creating Plot 4: Global vs local extrema...\")\n",
    "ax = fig.add_subplot(gs[1, 1])\n",
    "x_vals = np.linspace(-3, 3, 500)\n",
    "f_vals = np.sin(x_vals) + 0.1*x_vals**2\n",
    "\n",
    "ax.plot(x_vals, f_vals, 'b-', linewidth=2.5, label='f(x)')\n",
    "\n",
    "# Find local extrema (simplified)\n",
    "local_max_idx = argrelextrema(f_vals, np.greater)[0]\n",
    "local_min_idx = argrelextrema(f_vals, np.less)[0]\n",
    "\n",
    "for idx in local_max_idx:\n",
    "    ax.plot(x_vals[idx], f_vals[idx], 'yo', markersize=10, label='Local max' if idx == local_max_idx[0] else '')\n",
    "for idx in local_min_idx:\n",
    "    ax.plot(x_vals[idx], f_vals[idx], 'co', markersize=10, label='Local min' if idx == local_min_idx[0] else '')\n",
    "\n",
    "# Global extrema\n",
    "global_max_idx = np.argmax(f_vals)\n",
    "global_min_idx = np.argmin(f_vals)\n",
    "ax.plot(x_vals[global_max_idx], f_vals[global_max_idx], 'r*', markersize=20, label='Global max', zorder=5)\n",
    "ax.plot(x_vals[global_min_idx], f_vals[global_min_idx], 'g*', markersize=20, label='Global min', zorder=5)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('f(x)', fontsize=11)\n",
    "ax.set_title('Global vs Local Extrema', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Optimization landscape (2D contour)\n",
    "print(\"  Creating Plot 5: 2D optimization landscape...\")\n",
    "ax = fig.add_subplot(gs[2, 0])\n",
    "\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.linspace(-3, 3, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = (1 - X)**2 + 100*(Y - X**2)**2  # Rosenbrock function\n",
    "\n",
    "contour = ax.contour(X, Y, np.log(Z + 1), levels=20, cmap='viridis')\n",
    "ax.clabel(contour, inline=True, fontsize=8)\n",
    "ax.plot(1, 1, 'r*', markersize=20, label='Global minimum', zorder=5)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('y', fontsize=11)\n",
    "ax.set_title('2D Optimization Landscape\\nRosenbrock function (log scale)', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Critical points summary diagram\n",
    "print(\"  Creating Plot 6: Critical points summary...\")\n",
    "ax = fig.add_subplot(gs[2, 1])\n",
    "\n",
    "ax.text(0.5, 0.9, 'Finding & Classifying Critical Points', \n",
    "        ha='center', fontsize=13, fontweight='bold')\n",
    "\n",
    "steps = [\n",
    "    \"Step 1: Find f'(x)\",\n",
    "    \"Step 2: Solve f'(x) = 0 â†’ critical points\",\n",
    "    \"Step 3: Classify using tests:\",\n",
    "    \"  â€¢ First Derivative Test:\",\n",
    "    \"    - f' changes + to - â†’ Local MAX\",\n",
    "    \"    - f' changes - to + â†’ Local MIN\",\n",
    "    \"  â€¢ Second Derivative Test:\",\n",
    "    \"    - f''(c) > 0 â†’ Local MIN (âŒ£)\",\n",
    "    \"    - f''(c) < 0 â†’ Local MAX (âŒ¢)\",\n",
    "    \"    - f''(c) = 0 â†’ Inconclusive\",\n",
    "    \"Step 4: Check endpoints for global extrema\"\n",
    "]\n",
    "\n",
    "y_pos = 0.75\n",
    "for step in steps:\n",
    "    if step.startswith('Step'):\n",
    "        ax.text(0.05, y_pos, step, fontsize=10, fontweight='bold', family='monospace')\n",
    "    else:\n",
    "        ax.text(0.05, y_pos, step, fontsize=9, family='monospace')\n",
    "    y_pos -= 0.065\n",
    "\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ All 6 visualizations complete\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 4 COMPLETE: Critical Points and Extrema\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. FINDING GLOBAL EXTREMA ON CLOSED INTERVAL\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. FINDING GLOBAL EXTREMA ON CLOSED INTERVAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nFind global max/min of f(x) = xÂ³ - 3x on [-2, 3]\")\n",
    "\n",
    "f3_func = lambda x: x**3 - 3*x\n",
    "df3_func = lambda x: 3*x**2 - 3\n",
    "\n",
    "# Critical points in interval\n",
    "print(\"\\n  Step 1: Find critical points in [-2, 3]\")\n",
    "print(\"    f'(x) = 3xÂ² - 3 = 0\")\n",
    "print(\"    xÂ² = 1 â†’ x = Â±1\")\n",
    "print(\"    Critical points in interval: x = -1, 1\")\n",
    "\n",
    "# Evaluate at critical points and endpoints\n",
    "candidates = [-2, -1, 1, 3]\n",
    "print(f\"\\n  Step 2: Evaluate f at critical points and endpoints:\")\n",
    "print(f\"  {'x':>5} | {'f(x)':>8}\")\n",
    "print(\"  \" + \"-\"*16)\n",
    "\n",
    "values = {}\n",
    "for x_val in candidates:\n",
    "    f_val = f3_func(x_val)\n",
    "    values[x_val] = f_val\n",
    "    print(f\"  {x_val:>5} | {f_val:>8.2f}\")\n",
    "\n",
    "global_max = max(values.items(), key=lambda item: item[1])\n",
    "global_min = min(values.items(), key=lambda item: item[1])\n",
    "\n",
    "print(f\"\\n  Global maximum: f({global_max[0]}) = {global_max[1]:.2f}\")\n",
    "print(f\"  Global minimum: f({global_min[0]}) = {global_min[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. FINDING CRITICAL POINTS - Example 2\n",
    "\"\"\"\n",
    "\n",
    "# Example 2: f(x) = xâ´ - 4xÂ³\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example 2: f(x) = xâ´ - 4xÂ³\")\n",
    "\n",
    "f2 = x**4 - 4*x**3\n",
    "df2 = sp.diff(f2, x)\n",
    "ddf2 = sp.diff(df2, x)\n",
    "\n",
    "print(f\"  f'(x) = {df2}\")\n",
    "print(f\"  f''(x) = {ddf2}\")\n",
    "\n",
    "critical_points2 = sp.solve(df2, x)\n",
    "print(f\"\\n  Critical points: {critical_points2}\")\n",
    "\n",
    "print(\"\\n  Classification:\")\n",
    "for cp in critical_points2:\n",
    "    cp_val = float(cp)\n",
    "    f_val = float(f2.subs(x, cp))\n",
    "    ddf_val = float(ddf2.subs(x, cp))\n",
    "    \n",
    "    if ddf_val > 0:\n",
    "        classification = \"Local MINIMUM\"\n",
    "    elif ddf_val < 0:\n",
    "        classification = \"Local MAXIMUM\"\n",
    "    else:\n",
    "        classification = \"Inconclusive (use 1st derivative test)\"\n",
    "    \n",
    "    print(f\"    x = {cp_val}: f({cp_val:.0f}) = {f_val:.2f}, f''({cp_val:.0f}) = {ddf_val:.0f} â†’ {classification}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimization Applications\n",
    "\n",
    "### 5.1 Optimization Framework\n",
    "\n",
    "**Optimization** is the process of finding the best solution from a set of alternatives, typically by maximizing or minimizing an objective function.\n",
    "\n",
    "**General Optimization Problem:**\n",
    "$$\\text{Minimize (or Maximize)} \\quad f(x)$$\n",
    "$$\\text{Subject to} \\quad g_i(x) \\leq 0, \\quad i = 1, \\ldots, m$$\n",
    "$$\\quad \\quad \\quad \\quad h_j(x) = 0, \\quad j = 1, \\ldots, p$$\n",
    "\n",
    "Where:\n",
    "- $f(x)$: **Objective function** (what we want to optimize)\n",
    "- $g_i(x) \\leq 0$: **Inequality constraints**\n",
    "- $h_j(x) = 0$: **Equality constraints**\n",
    "- $x$: **Decision variables**\n",
    "\n",
    "**Unconstrained Optimization:** Only objective function, no constraints.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 Gradient Descent\n",
    "\n",
    "**Gradient Descent** is an iterative optimization algorithm for finding local minima of differentiable functions.\n",
    "\n",
    "**Algorithm:**\n",
    "1. Start with initial guess $x_0$\n",
    "2. Update: $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$\n",
    "3. Repeat until convergence\n",
    "\n",
    "Where:\n",
    "- $\\alpha$: **Learning rate** (step size)\n",
    "- $\\nabla f(x_k)$: **Gradient** (direction of steepest ascent)\n",
    "- Negative gradient: direction of steepest descent\n",
    "\n",
    "**Intuition:** Move in the direction opposite to the gradient to decrease function value.\n",
    "\n",
    "**Learning Rate Selection:**\n",
    "- Too small: slow convergence\n",
    "- Too large: oscillation or divergence\n",
    "- Adaptive methods: adjust $\\alpha$ dynamically\n",
    "\n",
    "**Variants:**\n",
    "- **Batch Gradient Descent:** Use entire dataset\n",
    "- **Stochastic Gradient Descent (SGD):** Use one sample at a time\n",
    "- **Mini-batch GD:** Use small batches\n",
    "\n",
    "**Example:** Minimize $f(x) = x^2 + 4x + 4 = (x+2)^2$\n",
    "- $f'(x) = 2x + 4$\n",
    "- Update: $x_{k+1} = x_k - \\alpha (2x_k + 4)$\n",
    "- Starting from $x_0 = 5$ with $\\alpha = 0.1$:\n",
    "  - $x_1 = 5 - 0.1(10 + 4) = 5 - 1.4 = 3.6$\n",
    "  - $x_2 = 3.6 - 0.1(7.2 + 4) = 3.6 - 1.12 = 2.48$\n",
    "  - $\\vdots$\n",
    "  - Converges to $x = -2$ (global minimum)\n",
    "\n",
    "---\n",
    "\n",
    "### 5.3 Newton's Method for Optimization\n",
    "\n",
    "**Newton's Method** uses second-order information (second derivatives) for faster convergence.\n",
    "\n",
    "**Update Rule:**\n",
    "$$x_{k+1} = x_k - \\frac{f'(x_k)}{f''(x_k)}$$\n",
    "\n",
    "**Matrix Form (multivariate):**\n",
    "$$\\mathbf{x}_{k+1} = \\mathbf{x}_k - [\\nabla^2 f(\\mathbf{x}_k)]^{-1} \\nabla f(\\mathbf{x}_k)$$\n",
    "\n",
    "Where $\\nabla^2 f$ is the **Hessian matrix** (matrix of second derivatives).\n",
    "\n",
    "**Advantages:**\n",
    "- Quadratic convergence (very fast near minimum)\n",
    "- Requires fewer iterations than gradient descent\n",
    "\n",
    "**Disadvantages:**\n",
    "- Requires computing and inverting Hessian (expensive for large $n$)\n",
    "- May not converge if starting point is far from minimum\n",
    "\n",
    "**Example:** Minimize $f(x) = x^2 - 4x + 4$\n",
    "- $f'(x) = 2x - 4$\n",
    "- $f''(x) = 2$\n",
    "- Update: $x_{k+1} = x_k - \\frac{2x_k - 4}{2} = x_k - (x_k - 2) = 2$\n",
    "- Converges in **one step** from any starting point!\n",
    "\n",
    "---\n",
    "\n",
    "### 5.4 Convex vs Non-Convex Optimization\n",
    "\n",
    "**Convex Function:** A function $f$ is convex if:\n",
    "$$f(\\lambda x_1 + (1-\\lambda)x_2) \\leq \\lambda f(x_1) + (1-\\lambda)f(x_2)$$\n",
    "for all $x_1, x_2$ and $\\lambda \\in [0, 1]$.\n",
    "\n",
    "**Graphically:** Line segment between any two points on the graph lies above the graph.\n",
    "\n",
    "**Properties:**\n",
    "- Any local minimum is a global minimum\n",
    "- Easier to optimize\n",
    "- Gradient descent guaranteed to find global minimum\n",
    "\n",
    "**Examples:**\n",
    "- Convex: $x^2$, $e^x$, $-\\log(x)$, linear functions\n",
    "- Non-convex: $\\sin(x)$, $x^3$, neural networks\n",
    "\n",
    "**Non-Convex Optimization:**\n",
    "- Multiple local minima\n",
    "- Gradient descent may get stuck in local minimum\n",
    "- Require sophisticated techniques (momentum, random restarts)\n",
    "\n",
    "---\n",
    "\n",
    "### 5.5 Applications in Machine Learning\n",
    "\n",
    "#### Loss Minimization\n",
    "\n",
    "**Goal:** Find parameters $\\theta$ that minimize loss function $L(\\theta)$.\n",
    "\n",
    "**Linear Regression:**\n",
    "$$L(\\theta) = \\frac{1}{2n}\\sum_{i=1}^n (y_i - \\theta^T x_i)^2$$\n",
    "- Gradient: $\\nabla L = -\\frac{1}{n}\\sum_{i=1}^n (y_i - \\theta^T x_i)x_i$\n",
    "- Update: $\\theta_{k+1} = \\theta_k + \\alpha \\cdot \\frac{1}{n}\\sum (y_i - \\theta_k^T x_i)x_i$\n",
    "\n",
    "**Logistic Regression:**\n",
    "$$L(\\theta) = -\\frac{1}{n}\\sum_{i=1}^n [y_i \\log(\\sigma(\\theta^T x_i)) + (1-y_i)\\log(1-\\sigma(\\theta^T x_i))]$$\n",
    "where $\\sigma(z) = \\frac{1}{1+e^{-z}}$ is the sigmoid function.\n",
    "\n",
    "#### Neural Network Training\n",
    "\n",
    "**Backpropagation = Chain Rule + Gradient Descent**\n",
    "1. Forward pass: compute loss\n",
    "2. Backward pass: compute gradients using chain rule\n",
    "3. Update weights using gradient descent\n",
    "\n",
    "**Training = Optimization:**\n",
    "- Minimize loss over training data\n",
    "- Navigate high-dimensional parameter space\n",
    "- Balance convergence speed and stability\n",
    "\n",
    "#### Hyperparameter Tuning\n",
    "\n",
    "**Goal:** Find optimal hyperparameters (learning rate, regularization strength, architecture choices).\n",
    "\n",
    "**Methods:**\n",
    "- Grid search: exhaustive evaluation\n",
    "- Random search: sample randomly\n",
    "- Bayesian optimization: model-based approach\n",
    "\n",
    "---\n",
    "\n",
    "### 5.6 Practical Considerations\n",
    "\n",
    "**Convergence Criteria:**\n",
    "1. $|x_{k+1} - x_k| < \\epsilon$ (small change in $x$)\n",
    "2. $|f(x_{k+1}) - f(x_k)| < \\epsilon$ (small change in $f$)\n",
    "3. $\\|\\nabla f(x_k)\\| < \\epsilon$ (gradient near zero)\n",
    "\n",
    "**Common Issues:**\n",
    "- **Oscillation:** Learning rate too large â†’ reduce $\\alpha$\n",
    "- **Slow convergence:** Learning rate too small â†’ increase $\\alpha$\n",
    "- **Local minima:** Non-convex function â†’ try different starting points\n",
    "- **Saddle points:** Gradient is zero but not a minimum â†’ use momentum\n",
    "\n",
    "**Improvements:**\n",
    "- **Momentum:** $v_{k+1} = \\beta v_k + \\nabla f(x_k)$, $x_{k+1} = x_k - \\alpha v_{k+1}$\n",
    "- **Adaptive learning rates:** Adam, RMSprop, Adagrad\n",
    "- **Line search:** Optimize $\\alpha$ at each step\n",
    "\n",
    "---\n",
    "\n",
    "### 5.7 Summary\n",
    "\n",
    "| Method | Update Rule | Convergence | Computational Cost | Best For |\n",
    "|--------|-------------|-------------|-------------------|----------|\n",
    "| Gradient Descent | $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$ | Linear | Low | Large-scale problems |\n",
    "| Newton's Method | $x_{k+1} = x_k - [f''(x_k)]^{-1}f'(x_k)$ | Quadratic | High | Small-scale, smooth functions |\n",
    "| SGD | $x_{k+1} = x_k - \\alpha \\nabla f(x_k; \\text{sample})$ | Noisy | Very Low | Large datasets, online learning |\n",
    "\n",
    "**Key Takeaway:** Derivatives are the foundation of optimization, which powers modern machine learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OPTIMIZATION APPLICATIONS - SECTION HEADER\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SECTION 5: OPTIMIZATION APPLICATIONS\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. GRADIENT DESCENT IMPLEMENTATION & EXAMPLES - Complete Demonstration\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. GRADIENT DESCENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def gradient_descent(f, df, x0, learning_rate=0.1, max_iters=100, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Gradient descent optimization algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    f : callable - objective function\n",
    "    df : callable - derivative of objective function\n",
    "    x0 : float - initial point\n",
    "    learning_rate : float - step size\n",
    "    max_iters : int - maximum iterations\n",
    "    tol : float - convergence tolerance\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    x_history : list - trajectory of x values\n",
    "    f_history : list - trajectory of function values\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    x_history = [x]\n",
    "    f_history = [f(x)]\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        # Compute gradient\n",
    "        grad = df(x)\n",
    "        \n",
    "        # Update rule\n",
    "        x_new = x - learning_rate * grad\n",
    "        \n",
    "        # Check convergence\n",
    "        if abs(x_new - x) < tol:\n",
    "            print(f\"  Converged in {i+1} iterations\")\n",
    "            break\n",
    "        \n",
    "        x = x_new\n",
    "        x_history.append(x)\n",
    "        f_history.append(f(x))\n",
    "    \n",
    "    return np.array(x_history), np.array(f_history)\n",
    "\n",
    "# Example 1: Simple quadratic\n",
    "print(\"\\nExample 1: f(x) = (x + 2)Â² (minimum at x = -2)\")\n",
    "\n",
    "f1 = lambda x: (x + 2)**2\n",
    "df1 = lambda x: 2*(x + 2)\n",
    "\n",
    "x_hist, f_hist = gradient_descent(f1, df1, x0=5.0, learning_rate=0.1)\n",
    "print(f\"  Initial: xâ‚€ = {x_hist[0]:.4f}, f(xâ‚€) = {f_hist[0]:.4f}\")\n",
    "print(f\"  Final: x = {x_hist[-1]:.4f}, f(x) = {f_hist[-1]:.4f}\")\n",
    "print(f\"  True minimum: x = -2.0, f(-2) = 0.0\")\n",
    "\n",
    "# Example 2: Effect of learning rate\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example 2: Learning rate comparison\")\n",
    "\n",
    "learning_rates = [0.01, 0.1, 0.5, 0.9]\n",
    "results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    x_hist_lr, f_hist_lr = gradient_descent(f1, df1, x0=5.0, learning_rate=lr, max_iters=50)\n",
    "    results[lr] = (x_hist_lr, f_hist_lr)\n",
    "    print(f\"  Î± = {lr:.2f}: {len(x_hist_lr)} iterations, final x = {x_hist_lr[-1]:.4f}\")\n",
    "\n",
    "print(\"\\n  All complete with different convergence speeds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. MACHINE LEARNING APPLICATION: Linear Regression with Gradient Descent\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. ML APPLICATION: LINEAR REGRESSION WITH GRADIENT DESCENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X_train = 2 * np.random.rand(n_samples, 1)\n",
    "y_train = 4 + 3 * X_train + np.random.randn(n_samples, 1)\n",
    "\n",
    "print(f\"\\n  Data: {n_samples} samples\")\n",
    "print(f\"  True model: y = 4 + 3x + noise\")\n",
    "\n",
    "def compute_mse(X, y, theta):\n",
    "    \"\"\"Mean squared error\"\"\"\n",
    "    predictions = X.dot(theta)\n",
    "    errors = predictions - y\n",
    "    return (1/(2*len(y))) * np.sum(errors**2)\n",
    "\n",
    "def compute_mse_gradient(X, y, theta):\n",
    "    \"\"\"Gradient of MSE\"\"\"\n",
    "    predictions = X.dot(theta)\n",
    "    errors = predictions - y\n",
    "    return (1/len(y)) * X.T.dot(errors)\n",
    "\n",
    "# Add intercept term\n",
    "X_b = np.c_[np.ones((n_samples, 1)), X_train]\n",
    "\n",
    "# Initialize parameters\n",
    "theta = np.random.randn(2, 1)\n",
    "\n",
    "# Gradient descent for linear regression\n",
    "learning_rate = 0.1\n",
    "n_iterations = 1000\n",
    "theta_history = [theta.copy()]\n",
    "mse_history = [compute_mse(X_b, y_train, theta)]\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = compute_mse_gradient(X_b, y_train, theta)\n",
    "    theta = theta - learning_rate * gradients\n",
    "    \n",
    "    if iteration % 100 == 0:\n",
    "        theta_history.append(theta.copy())\n",
    "        mse_history.append(compute_mse(X_b, y_train, theta))\n",
    "\n",
    "print(f\"\\n  Initial parameters: Î¸â‚€ = {theta_history[0][0,0]:.4f}, Î¸â‚ = {theta_history[0][1,0]:.4f}\")\n",
    "print(f\"  Final parameters: Î¸â‚€ = {theta[0,0]:.4f}, Î¸â‚ = {theta[1,0]:.4f}\")\n",
    "print(f\"  True parameters: Î¸â‚€ = 4.0, Î¸â‚ = 3.0\")\n",
    "print(f\"  Initial MSE: {mse_history[0]:.4f}\")\n",
    "print(f\"  Final MSE: {mse_history[-1]:.4f}\")\n",
    "print(\"\\n  âœ“ Successfully learned parameters close to true values!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4. COMPREHENSIVE VISUALIZATIONS (8 plots) - Section 5 Complete\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. COMPREHENSIVE VISUALIZATIONS (8 plots)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.35)\n",
    "\n",
    "# Plot 1: Gradient descent trajectory\n",
    "print(\"\\n  Creating Plot 1: Gradient descent trajectory...\")\n",
    "ax = fig.add_subplot(gs[0, 0])\n",
    "x_vals = np.linspace(-3, 6, 500)\n",
    "ax.plot(x_vals, f1(x_vals), 'b-', linewidth=2, label='f(x) = (x+2)Â²')\n",
    "ax.plot(x_hist, f_hist, 'ro-', markersize=6, linewidth=1.5, label='GD trajectory', alpha=0.7)\n",
    "ax.plot(x_hist[0], f_hist[0], 'go', markersize=12, label='Start', zorder=5)\n",
    "ax.plot(x_hist[-1], f_hist[-1], 'r*', markersize=15, label='Optimum', zorder=5)\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('f(x)', fontsize=11)\n",
    "ax.set_title('Gradient Descent Trajectory\\nÎ±=0.1, converges to x=-2', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Learning rate comparison\n",
    "print(\"  Creating Plot 2: Learning rate effects...\")\n",
    "ax = fig.add_subplot(gs[0, 1])\n",
    "for lr in learning_rates:\n",
    "    x_hist_lr, f_hist_lr = results[lr]\n",
    "    ax.plot(range(len(f_hist_lr)), f_hist_lr, '-o', markersize=3, label=f'Î±={lr}', linewidth=1.5)\n",
    "ax.set_xlabel('Iteration', fontsize=11)\n",
    "ax.set_ylabel('f(x)', fontsize=11)\n",
    "ax.set_title('Learning Rate Comparison\\nFaster convergence with larger Î±', \n",
    "             fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Plot 3: Newton vs Gradient Descent\n",
    "print(\"  Creating Plot 3: Newton vs GD comparison...\")\n",
    "ax = fig.add_subplot(gs[0, 2])\n",
    "ax.plot(range(len(f_gd)), f_gd, 'b-o', markersize=3, label='Gradient Descent', linewidth=1.5, alpha=0.7)\n",
    "ax.plot(range(len(f_newton)), f_newton, 'r-s', markersize=5, label=\"Newton's Method\", linewidth=2)\n",
    "ax.set_xlabel('Iteration', fontsize=11)\n",
    "ax.set_ylabel('f(x)', fontsize=11)\n",
    "ax.set_title(\"Newton's Method vs Gradient Descent\\nNewton converges much faster!\", \n",
    "             fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Plot 4: Linear regression fit\n",
    "print(\"  Creating Plot 4: Linear regression...\")\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "ax.scatter(X_train, y_train, alpha=0.5, s=30, label='Training data')\n",
    "x_plot = np.linspace(0, 2, 100).reshape(-1, 1)\n",
    "x_plot_b = np.c_[np.ones((100, 1)), x_plot]\n",
    "y_pred = x_plot_b.dot(theta)\n",
    "ax.plot(x_plot, y_pred, 'r-', linewidth=2.5, label=f'Fitted: y={theta[0,0]:.2f}+{theta[1,0]:.2f}x')\n",
    "ax.plot(x_plot, 4 + 3*x_plot, 'g--', linewidth=2, label='True: y=4+3x', alpha=0.7)\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('y', fontsize=11)\n",
    "ax.set_title('Linear Regression with Gradient Descent\\nLearned parameters match true model', \n",
    "             fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: MSE convergence\n",
    "print(\"  Creating Plot 5: MSE convergence...\")\n",
    "ax = fig.add_subplot(gs[1, 1])\n",
    "ax.plot(np.arange(0, n_iterations+1, 100), mse_history, 'b-o', linewidth=2, markersize=6)\n",
    "ax.set_xlabel('Iteration', fontsize=11)\n",
    "ax.set_ylabel('Mean Squared Error', fontsize=11)\n",
    "ax.set_title('Training Loss Convergence\\nMSE decreases as model learns', fontsize=11, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Convex vs non-convex\n",
    "print(\"  Creating Plot 6: Convex vs non-convex...\")\n",
    "ax = fig.add_subplot(gs[1, 2])\n",
    "x_vals = np.linspace(-6, 6, 500)\n",
    "f_convex = lambda x: x**2\n",
    "f_nonconvex = lambda x: np.sin(x) + 0.1*x**2\n",
    "ax.plot(x_vals, f_convex(x_vals), 'b-', linewidth=2, label='Convex: xÂ²')\n",
    "ax.plot(x_vals, f_nonconvex(x_vals), 'r-', linewidth=2, label='Non-convex: sin(x)+0.1xÂ²')\n",
    "ax.axhline(0, color='black', linewidth=0.8)\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('f(x)', fontsize=11)\n",
    "ax.set_title('Convex vs Non-Convex Functions', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 7: Rosenbrock function contour with GD trajectory\n",
    "print(\"  Creating Plot 7: Rosenbrock function...\")\n",
    "ax = fig.add_subplot(gs[2, 0])\n",
    "\n",
    "def rosenbrock(x, y):\n",
    "    return (1 - x)**2 + 100*(y - x**2)**2\n",
    "\n",
    "def rosenbrock_gradient(x, y):\n",
    "    dx = -2*(1 - x) - 400*x*(y - x**2)\n",
    "    dy = 200*(y - x**2)\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "# Optimize on Rosenbrock\n",
    "point = np.array([-1.0, 1.0])\n",
    "alpha_gd = 0.001\n",
    "trajectory = [point.copy()]\n",
    "\n",
    "for i in range(1000):\n",
    "    grad = rosenbrock_gradient(point[0], point[1])\n",
    "    point = point - alpha_gd * grad\n",
    "    if i % 100 == 0:\n",
    "        trajectory.append(point.copy())\n",
    "\n",
    "trajectory = np.array(trajectory)\n",
    "\n",
    "x = np.linspace(-2, 2, 100)\n",
    "y = np.linspace(-1, 3, 100)\n",
    "X_mesh, Y_mesh = np.meshgrid(x, y)\n",
    "Z = rosenbrock(X_mesh, Y_mesh)\n",
    "contour = ax.contour(X_mesh, Y_mesh, np.log(Z + 1), levels=20, cmap='viridis')\n",
    "ax.plot(trajectory[:, 0], trajectory[:, 1], 'r-o', markersize=4, linewidth=2, label='GD path')\n",
    "ax.plot(1, 1, 'r*', markersize=20, label='Global minimum', zorder=5)\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('y', fontsize=11)\n",
    "ax.set_title('Rosenbrock Function Optimization', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 8: Gradient descent algorithm summary\n",
    "print(\"  Creating Plot 8: Algorithm summary...\")\n",
    "ax = fig.add_subplot(gs[2, 1:])\n",
    "ax.text(0.5, 0.95, 'Gradient Descent Algorithm', ha='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "algorithm_steps = [\n",
    "    \"1. Initialize: Choose starting point xâ‚€ and learning rate Î±\",\n",
    "    \"\",\n",
    "    \"2. Repeat until convergence:\",\n",
    "    \"   a) Compute gradient: g = âˆ‡f(xâ‚–)\",\n",
    "    \"   b) Update: xâ‚–â‚Šâ‚ = xâ‚– - Î±Â·g\",\n",
    "    \"   c) Check: |xâ‚–â‚Šâ‚ - xâ‚–| < Îµ ?\",\n",
    "    \"\",\n",
    "    \"3. Return: xâ‚–â‚Šâ‚ as optimal solution\",\n",
    "    \"\",\n",
    "    \"Key Parameters:\",\n",
    "    \"  â€¢ Î± (learning rate): Controls step size\",\n",
    "    \"    - Too small: slow convergence\",\n",
    "    \"    - Too large: oscillation/divergence\",\n",
    "    \"    - Typical: 0.001 to 0.1\",\n",
    "    \"\",\n",
    "    \"Applications:\",\n",
    "    \"  âœ“ Neural network training (backprop + GD)\",\n",
    "    \"  âœ“ Linear/logistic regression\",\n",
    "    \"  âœ“ Support vector machines\",\n",
    "    \"  âœ“ Any differentiable optimization problem\"\n",
    "]\n",
    "\n",
    "y_pos = 0.85\n",
    "for step in algorithm_steps:\n",
    "    if step.startswith(('1.', '2.', '3.', 'Key', 'Applications')):\n",
    "        ax.text(0.05, y_pos, step, fontsize=10, fontweight='bold', family='monospace')\n",
    "    else:\n",
    "        ax.text(0.05, y_pos, step, fontsize=9, family='monospace')\n",
    "    y_pos -= 0.04\n",
    "\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ All 8 visualizations complete\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 5 COMPLETE: Optimization Applications\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. NEWTON'S METHOD vs GRADIENT DESCENT - Comparison\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. NEWTON'S METHOD\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def newtons_method(f, df, ddf, x0, max_iters=20, tol=1e-6):\n",
    "    \"\"\"Newton's method for optimization.\"\"\"\n",
    "    x = x0\n",
    "    x_history = [x]\n",
    "    f_history = [f(x)]\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        # Newton's update\n",
    "        x_new = x - df(x) / ddf(x)\n",
    "        \n",
    "        if abs(x_new - x) < tol:\n",
    "            print(f\"  Converged in {i+1} iterations\")\n",
    "            break\n",
    "        \n",
    "        x = x_new\n",
    "        x_history.append(x)\n",
    "        f_history.append(f(x))\n",
    "    \n",
    "    return np.array(x_history), np.array(f_history)\n",
    "\n",
    "# Example: Compare Newton vs Gradient Descent\n",
    "print(\"\\nCompare Newton's method vs Gradient Descent\")\n",
    "print(\"  Function: f(x) = xâ´ - 3xÂ³ + 2\")\n",
    "\n",
    "f2 = lambda x: x**4 - 3*x**3 + 2\n",
    "df2 = lambda x: 4*x**3 - 9*x**2\n",
    "ddf2 = lambda x: 12*x**2 - 18*x\n",
    "\n",
    "print(\"\\n  Gradient Descent (Î±=0.01):\")\n",
    "x_gd, f_gd = gradient_descent(f2, df2, x0=3.0, learning_rate=0.01, max_iters=1000)\n",
    "print(f\"    Final x = {x_gd[-1]:.6f}, f(x) = {f_gd[-1]:.6f}, iterations = {len(x_gd)}\")\n",
    "\n",
    "print(\"\\n  Newton's Method:\")\n",
    "x_newton, f_newton = newtons_method(f2, df2, ddf2, x0=3.0)\n",
    "print(f\"    Final x = {x_newton[-1]:.6f}, f(x) = {f_newton[-1]:.6f}, iterations = {len(x_newton)}\")\n",
    "\n",
    "print(\"\\n  âœ“ Newton's method converges much faster!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 11 (BONUS): Neural Network Backpropagation\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROBLEM 11 (BONUS): Neural Network Backpropagation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "x_nn = 3\n",
    "w1 = 0.5\n",
    "b1 = 1.0\n",
    "alpha_nn = 0.1\n",
    "\n",
    "print(f\"\\nGiven: x={x_nn}, wâ‚={w1}, bâ‚={b1}\")\n",
    "print(\"Network: L = (aâ‚)Â² where aâ‚ = ReLU(wâ‚x + bâ‚)\")\n",
    "\n",
    "print(\"\\na) Forward pass:\")\n",
    "z1 = w1 * x_nn + b1\n",
    "print(f\"   zâ‚ = wâ‚Â·x + bâ‚ = {w1}Â·{x_nn} + {b1} = {z1}\")\n",
    "a1 = max(0, z1)\n",
    "print(f\"   aâ‚ = ReLU(zâ‚) = max(0, {z1}) = {a1}\")\n",
    "L = a1**2\n",
    "print(f\"   L = (aâ‚)Â² = {a1}Â² = {L}\")\n",
    "\n",
    "print(\"\\nb) Backpropagation using chain rule:\")\n",
    "print(\"   âˆ‚L/âˆ‚wâ‚ = (âˆ‚L/âˆ‚aâ‚)Â·(âˆ‚aâ‚/âˆ‚zâ‚)Â·(âˆ‚zâ‚/âˆ‚wâ‚)\")\n",
    "print(f\"   âˆ‚L/âˆ‚aâ‚ = 2aâ‚ = 2Â·{a1} = {2*a1}\")\n",
    "relu_grad = 1 if z1 > 0 else 0\n",
    "print(f\"   âˆ‚aâ‚/âˆ‚zâ‚ = ReLU'(zâ‚) = {relu_grad} (since zâ‚={z1}>0)\")\n",
    "print(f\"   âˆ‚zâ‚/âˆ‚wâ‚ = x = {x_nn}\")\n",
    "grad_w1 = 2*a1 * relu_grad * x_nn\n",
    "print(f\"   âˆ‚L/âˆ‚wâ‚ = {2*a1}Â·{relu_grad}Â·{x_nn} = {grad_w1}\")\n",
    "\n",
    "print(\"\\nc) Update wâ‚:\")\n",
    "w1_new = w1 - alpha_nn * grad_w1\n",
    "print(f\"   wâ‚_new = wâ‚ - Î±Â·(âˆ‚L/âˆ‚wâ‚) = {w1} - {alpha_nn}Â·{grad_w1} = {w1_new}\")\n",
    "\n",
    "print(\"\\nd) Numerical verification:\")\n",
    "h = 1e-7\n",
    "L_plus = (max(0, (w1+h)*x_nn + b1))**2\n",
    "L_original = (max(0, w1*x_nn + b1))**2\n",
    "numerical_grad = (L_plus - L_original) / h\n",
    "print(f\"   Numerical gradient â‰ˆ {numerical_grad:.4f}\")\n",
    "print(f\"   Analytical gradient = {grad_w1:.4f}\")\n",
    "print(f\"   âœ“ Match (difference: {abs(numerical_grad - grad_w1):.10f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ“ ALL 11 PRACTICE PROBLEMS COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 10: Newton's Method\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROBLEM 10: Newton's Method\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nFind root of f(x) = xÂ³ - 2x - 5 starting from xâ‚€=2\")\n",
    "\n",
    "f10 = lambda x: x**3 - 2*x - 5\n",
    "df10 = lambda x: 3*x**2 - 2\n",
    "\n",
    "print(\"\\na) Update formula: x_{n+1} = x_n - f(x_n)/f'(x_n)\")\n",
    "print(\"   = x_n - (x_nÂ³ - 2x_n - 5)/(3x_nÂ² - 2)\")\n",
    "\n",
    "print(\"\\nb) Iterations:\")\n",
    "x_newton = 2.0\n",
    "for i in range(4):\n",
    "    f_val = f10(x_newton)\n",
    "    df_val = df10(x_newton)\n",
    "    x_new = x_newton - f_val / df_val\n",
    "    print(f\"   Iteration {i+1}: x={x_newton:.6f}, f(x)={f_val:.6f}, x_new={x_new:.6f}\")\n",
    "    x_newton = x_new\n",
    "\n",
    "print(f\"\\nc) âœ“ Verification: f({x_newton:.6f}) = {f10(x_newton):.8f} â‰ˆ 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 9: Implicit Differentiation\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROBLEM 9: Implicit Differentiation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nEquation: xÂ² + yÂ² = 25\")\n",
    "print(\"\\nSolution:\")\n",
    "print(\"  Differentiate both sides: 2x + 2y(dy/dx) = 0\")\n",
    "print(\"  Solve for dy/dx: dy/dx = -x/y\")\n",
    "print(\"\\n  At point (3, 4):\")\n",
    "print(\"  dy/dx = -3/4 = -0.75\")\n",
    "print(\"\\n  âœ“ Slope of tangent line at (3,4) is -3/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 8: Machine Learning - Linear Regression with Gradient Descent\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROBLEM 8: Linear Regression with Gradient Descent\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X = np.array([1, 2, 3])\n",
    "y = np.array([3, 5, 7])\n",
    "n = len(X)\n",
    "\n",
    "print(f\"\\nData: {list(zip(X, y))}\")\n",
    "\n",
    "print(\"\\na) Loss: L(Î¸) = (1/2n)Î£(yáµ¢ - Î¸xáµ¢)Â²\")\n",
    "print(\"   Gradient: âˆ‚L/âˆ‚Î¸ = -(1/n)Î£(yáµ¢ - Î¸xáµ¢)xáµ¢\")\n",
    "\n",
    "print(\"\\nb) Gradient descent with Î±=0.1, Î¸â‚€=0:\")\n",
    "theta = 0.0\n",
    "for iter in range(3):\n",
    "    predictions = theta * X\n",
    "    errors = y - predictions\n",
    "    gradient = -(1/n) * np.sum(errors * X)\n",
    "    theta_new = theta - 0.1 * gradient\n",
    "    loss = (1/(2*n)) * np.sum(errors**2)\n",
    "    print(f\"   Iteration {iter+1}: Î¸={theta:.4f}, gradient={gradient:.4f}, loss={loss:.4f}, Î¸_new={theta_new:.4f}\")\n",
    "    theta = theta_new\n",
    "\n",
    "print(f\"\\n   After 3 iterations: Î¸ = {theta:.4f}\")\n",
    "\n",
    "print(\"\\nc) Optimal Î¸ (analytical solution):\")\n",
    "theta_opt = np.sum(X * y) / np.sum(X * X)\n",
    "print(f\"   Î¸* = Î£(xáµ¢yáµ¢) / Î£(xáµ¢Â²) = {np.sum(X*y)} / {np.sum(X*X)} = {theta_opt:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 7: Related Rates - Sliding Ladder\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROBLEM 7: Related Rates - Sliding Ladder\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nLadder: 10m long, bottom sliding away at 1 m/s\")\n",
    "print(\"Find: How fast is top sliding down when bottom is 6m from wall?\")\n",
    "\n",
    "print(\"\\nSolution:\")\n",
    "print(\"  Let x = distance from wall to bottom, y = height of top\")\n",
    "print(\"  Pythagorean theorem: xÂ² + yÂ² = 100\")\n",
    "print(\"  Differentiate: 2x(dx/dt) + 2y(dy/dt) = 0\")\n",
    "print(\"  Solve for dy/dt: dy/dt = -x(dx/dt) / y\")\n",
    "print(\"\\n  Given: dx/dt = 1 m/s, x = 6m\")\n",
    "print(\"  Find y: 6Â² + yÂ² = 100  â†’  y = 8m\")\n",
    "print(\"  dy/dt = -6(1) / 8 = -0.75 m/s\")\n",
    "print(\"\\n  âœ“ Answer: Top sliding down at 0.75 m/s (negative = downward)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 6: Gradient Descent on polynomial\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROBLEM 6: Gradient Descent on f(x) = xâ´ - 3xÂ² + 2\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "f6_func = lambda x: x**4 - 3*x**2 + 2\n",
    "df6_func = lambda x: 4*x**3 - 6*x\n",
    "\n",
    "print(\"\\na) f'(x) = 4xÂ³ - 6x\")\n",
    "\n",
    "print(\"\\nb-c) Implementing gradient descent:\")\n",
    "x_gd = 2.0\n",
    "alpha = 0.1\n",
    "iterations = 0\n",
    "tolerance = 1e-6\n",
    "\n",
    "gd_history = [x_gd]\n",
    "\n",
    "while iterations < 1000:\n",
    "    x_new = x_gd - alpha * df6_func(x_gd)\n",
    "    if abs(x_new - x_gd) < tolerance:\n",
    "        break\n",
    "    x_gd = x_new\n",
    "    gd_history.append(x_gd)\n",
    "    iterations += 1\n",
    "\n",
    "print(f\"   Converged in {iterations} iterations\")\n",
    "print(f\"   Final x = {x_gd:.6f}\")\n",
    "print(f\"   f(x) = {f6_func(x_gd):.6f}\")\n",
    "\n",
    "print(\"\\nd) Analysis:\")\n",
    "print(f\"   Starting from xâ‚€=2, algorithm found local minimum at xâ‰ˆ{x_gd:.4f}\")\n",
    "print(\"   f(x) has 3 critical points: x â‰ˆ -1.22, 0, 1.22\")\n",
    "print(\"   Global minimum is at x â‰ˆ Â±1.22 with f(x) â‰ˆ -0.25\")\n",
    "print(\"   Converged to local minimum at xâ‰ˆ1.22 (depends on starting point)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 5: Optimization - Rectangular Field\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROBLEM 5: Optimization - Rectangular Field\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nFarmer has 200m fencing, one side is river (no fence needed)\")\n",
    "print(\"\\nLet x = width (perpendicular to river), y = length (parallel to river)\")\n",
    "print(\"Constraint: 2x + y = 200  â†’  y = 200 - 2x\")\n",
    "print(\"Area: A(x) = xÂ·y = x(200 - 2x) = 200x - 2xÂ²\")\n",
    "\n",
    "print(\"\\na) A(x) = 200x - 2xÂ²\")\n",
    "\n",
    "print(\"\\nb) Find critical points:\")\n",
    "print(\"   A'(x) = 200 - 4x = 0\")\n",
    "print(\"   x = 50\")\n",
    "\n",
    "print(\"\\nc) Second derivative test:\")\n",
    "print(\"   A''(x) = -4 < 0  â†’  Maximum at x=50\")\n",
    "\n",
    "print(\"\\nd) Maximum area:\")\n",
    "print(\"   x = 50m, y = 200 - 2(50) = 100m\")\n",
    "print(\"   A_max = 50 Ã— 100 = 5000 mÂ²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 4: Critical Points and Classification\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROBLEM 4: Critical Points and Classification\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nf(x) = xâ´ - 4xÂ³ + 10\")\n",
    "f4 = x**4 - 4*x**3 + 10\n",
    "df4 = sp.diff(f4, x)\n",
    "ddf4 = sp.diff(df4, x)\n",
    "\n",
    "print(f\"\\na) f'(x) = {df4}\")\n",
    "critical_pts = sp.solve(df4, x)\n",
    "print(f\"   Critical points: {critical_pts}\")\n",
    "\n",
    "print(\"\\nb) Second derivative test:\")\n",
    "print(f\"   f''(x) = {ddf4}\")\n",
    "for cp in critical_pts:\n",
    "    ddf_val = ddf4.subs(x, cp)\n",
    "    f_val = f4.subs(x, cp)\n",
    "    if ddf_val > 0:\n",
    "        classification = \"Local MINIMUM\"\n",
    "    elif ddf_val < 0:\n",
    "        classification = \"Local MAXIMUM\"\n",
    "    else:\n",
    "        classification = \"Inconclusive\"\n",
    "    print(f\"   x = {cp}: f''({cp}) = {ddf_val} â†’ {classification}, f({cp}) = {f_val}\")\n",
    "\n",
    "print(\"\\nc) Global extrema on [-1, 4]:\")\n",
    "endpoints = [-1, 4]\n",
    "candidates = list(critical_pts) + endpoints\n",
    "print(\"   Evaluating at critical points and endpoints:\")\n",
    "for pt in candidates:\n",
    "    val = f4.subs(x, pt)\n",
    "    print(f\"   f({pt}) = {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 3: Chain Rule Applications\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROBLEM 3: Chain Rule Applications\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# a)\n",
    "print(\"\\na) f(x) = (2xÂ² + 3x + 1)âµ\")\n",
    "f3a = (2*x**2 + 3*x + 1)**5\n",
    "df3a = sp.diff(f3a, x)\n",
    "print(f\"   f'(x) = {df3a}\")\n",
    "print(\"   Using chain rule: 5(2xÂ²+3x+1)â´ Â· (4x+3)\")\n",
    "\n",
    "# b)\n",
    "print(\"\\nb) g(x) = e^(xÂ³+2x)\")\n",
    "f3b = sp.exp(x**3 + 2*x)\n",
    "df3b = sp.diff(f3b, x)\n",
    "print(f\"   g'(x) = {df3b}\")\n",
    "\n",
    "# c)\n",
    "print(\"\\nc) h(x) = sin(xÂ²+1)\")\n",
    "f3c = sp.sin(x**2 + 1)\n",
    "df3c = sp.diff(f3c, x)\n",
    "print(f\"   h'(x) = {df3c}\")\n",
    "\n",
    "# d)\n",
    "print(\"\\nd) k(x) = ln(âˆš(xÂ²+1))\")\n",
    "f3d = sp.log(sp.sqrt(x**2 + 1))\n",
    "df3d = sp.diff(f3d, x)\n",
    "df3d_simplified = sp.simplify(df3d)\n",
    "print(f\"   k'(x) = {df3d_simplified}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 2: Differentiation Rules (Power, Product, Quotient)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROBLEM 2: Differentiation Rules\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# a) Power rule\n",
    "print(\"\\na) f(x) = 3xâµ - 2xÂ³ + 7x - 4\")\n",
    "f2a = 3*x**5 - 2*x**3 + 7*x - 4\n",
    "df2a = sp.diff(f2a, x)\n",
    "print(f\"   f'(x) = {df2a}\")\n",
    "\n",
    "# b) Product rule\n",
    "print(\"\\nb) g(x) = xÂ²eË£\")\n",
    "f2b = x**2 * sp.exp(x)\n",
    "df2b = sp.diff(f2b, x)\n",
    "print(f\"   g'(x) = {df2b}\")\n",
    "print(\"   Using product rule: (xÂ²)'(eË£) + (xÂ²)(eË£)' = 2xÂ·eË£ + xÂ²Â·eË£ = eË£(xÂ²+2x)\")\n",
    "\n",
    "# c) Quotient rule\n",
    "print(\"\\nc) h(x) = (xÂ²+1)/(x-1)\")\n",
    "f2c = (x**2 + 1) / (x - 1)\n",
    "df2c = sp.diff(f2c, x)\n",
    "df2c_simplified = sp.simplify(df2c)\n",
    "print(f\"   h'(x) = {df2c_simplified}\")\n",
    "print(\"   Using quotient rule: [(2x)(x-1) - (xÂ²+1)(1)] / (x-1)Â²\")\n",
    "\n",
    "# d) Product + trig\n",
    "print(\"\\nd) k(x) = sin(x)cos(x)\")\n",
    "f2d = sp.sin(x) * sp.cos(x)\n",
    "df2d = sp.diff(f2d, x)\n",
    "df2d_simplified = sp.simplify(df2d)\n",
    "print(f\"   k'(x) = {df2d_simplified}\")\n",
    "print(\"   Note: sin(x)cos(x) = Â½sin(2x), so derivative is cos(2x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 1: First Principles\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROBLEM 1: Derivative from First Principles\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nFind derivative of f(x) = âˆš(x+1) using limit definition:\")\n",
    "print(\"\\nSolution:\")\n",
    "print(\"  f'(x) = lim[hâ†’0] [âˆš(x+h+1) - âˆš(x+1)] / h\")\n",
    "print(\"  Multiply by conjugate: [âˆš(x+h+1) + âˆš(x+1)] / [âˆš(x+h+1) + âˆš(x+1)]\")\n",
    "print(\"  = lim[hâ†’0] [(x+h+1) - (x+1)] / [h(âˆš(x+h+1) + âˆš(x+1))]\")\n",
    "print(\"  = lim[hâ†’0] h / [h(âˆš(x+h+1) + âˆš(x+1))]\")\n",
    "print(\"  = lim[hâ†’0] 1 / [âˆš(x+h+1) + âˆš(x+1)]\")\n",
    "print(\"  = 1 / [2âˆš(x+1)]\")\n",
    "\n",
    "# Verify with SymPy\n",
    "x = sp.Symbol('x')\n",
    "f1 = sp.sqrt(x + 1)\n",
    "df1 = sp.diff(f1, x)\n",
    "print(f\"\\n  âœ“ SymPy verification: f'(x) = {df1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PRACTICE PROBLEMS - DETAILED SOLUTIONS\n",
    "Section header\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PRACTICE PROBLEMS - SOLUTIONS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nâœ“ 11 comprehensive problems covering all derivative concepts\")\n",
    "print(\"  - First principles and limit definition\")\n",
    "print(\"  - Differentiation rules (power, product, quotient)\")\n",
    "print(\"  - Chain rule applications\")\n",
    "print(\"  - Critical points and optimization\")\n",
    "print(\"  - Gradient descent and Newton's method\")\n",
    "print(\"  - Related rates and implicit differentiation\")\n",
    "print(\"  - Machine learning applications (linear regression, backpropagation)\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Problems\n",
    "\n",
    "Test your understanding of derivatives with these comprehensive problems covering all topics from this week.\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 1: Computing Derivatives from First Principles\n",
    "\n",
    "Compute the derivative of $f(x) = \\sqrt{x+1}$ using the limit definition:\n",
    "$$f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$$\n",
    "\n",
    "**Hint:** Multiply by the conjugate to simplify.\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 2: Differentiation Rules\n",
    "\n",
    "Find the derivatives of the following functions:\n",
    "\n",
    "a) $f(x) = 3x^5 - 2x^3 + 7x - 4$\n",
    "\n",
    "b) $g(x) = x^2 e^x$\n",
    "\n",
    "c) $h(x) = \\frac{x^2 + 1}{x - 1}$\n",
    "\n",
    "d) $k(x) = \\sin(x) \\cos(x)$\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 3: Chain Rule Applications\n",
    "\n",
    "Compute the derivatives using the chain rule:\n",
    "\n",
    "a) $f(x) = (2x^2 + 3x + 1)^5$\n",
    "\n",
    "b) $g(x) = e^{x^3 + 2x}$\n",
    "\n",
    "c) $h(x) = \\sin(x^2 + 1)$\n",
    "\n",
    "d) $k(x) = \\ln(\\sqrt{x^2 + 1})$\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 4: Critical Points and Classification\n",
    "\n",
    "For $f(x) = x^4 - 4x^3 + 10$:\n",
    "\n",
    "a) Find all critical points by solving $f'(x) = 0$\n",
    "\n",
    "b) Classify each critical point using the second derivative test\n",
    "\n",
    "c) Determine the global maximum and minimum on the interval $[-1, 4]$\n",
    "\n",
    "d) Sketch the function showing all critical points\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 5: Optimization Problem\n",
    "\n",
    "A farmer has 200 meters of fencing to enclose a rectangular field adjacent to a river (so one side doesn't need fencing). What dimensions will maximize the area?\n",
    "\n",
    "a) Express the area $A$ as a function of one variable\n",
    "\n",
    "b) Find the critical points\n",
    "\n",
    "c) Verify that your answer gives a maximum\n",
    "\n",
    "d) What is the maximum area?\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 6: Gradient Descent Implementation\n",
    "\n",
    "Consider $f(x) = x^4 - 3x^2 + 2$:\n",
    "\n",
    "a) Compute $f'(x)$ symbolically\n",
    "\n",
    "b) Implement gradient descent starting from $x_0 = 2$ with $\\alpha = 0.1$\n",
    "\n",
    "c) How many iterations until convergence (tolerance $10^{-6}$)?\n",
    "\n",
    "d) Does the algorithm find the global minimum? Why or why not?\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 7: Related Rates\n",
    "\n",
    "A ladder 10 meters long rests against a vertical wall. If the bottom slides away from the wall at 1 m/s, how fast is the top sliding down when the bottom is 6 meters from the wall?\n",
    "\n",
    "**Hint:** Use the Pythagorean theorem and implicit differentiation.\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 8: Machine Learning Application\n",
    "\n",
    "Consider a simple linear regression problem with loss function:\n",
    "$$L(\\theta) = \\frac{1}{2n}\\sum_{i=1}^n (y_i - \\theta x_i)^2$$\n",
    "\n",
    "Given data: $(x_1, y_1) = (1, 3)$, $(x_2, y_2) = (2, 5)$, $(x_3, y_3) = (3, 7)$\n",
    "\n",
    "a) Compute the gradient $\\frac{\\partial L}{\\partial \\theta}$ symbolically\n",
    "\n",
    "b) Starting from $\\theta_0 = 0$, perform 3 iterations of gradient descent with $\\alpha = 0.1$\n",
    "\n",
    "c) What is the optimal value of $\\theta$ (you can solve analytically)?\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 9: Implicit Differentiation\n",
    "\n",
    "Find $\\frac{dy}{dx}$ for the equation:\n",
    "$$x^2 + y^2 = 25$$\n",
    "\n",
    "Then find the slope of the tangent line at the point $(3, 4)$.\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 10: Newton's Method\n",
    "\n",
    "Use Newton's method to find the root of $f(x) = x^3 - 2x - 5$ starting from $x_0 = 2$.\n",
    "\n",
    "a) Write the Newton's method update formula for this specific function\n",
    "\n",
    "b) Perform 4 iterations manually (or with code)\n",
    "\n",
    "c) Verify your answer by checking $f(x) \\approx 0$\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 11 (Bonus): Neural Network Backpropagation\n",
    "\n",
    "Consider a simple neural network: $L = (a_1)^2$ where $a_1 = \\text{ReLU}(w_1 x + b_1)$\n",
    "\n",
    "Given: $x = 3$, $w_1 = 0.5$, $b_1 = 1$\n",
    "\n",
    "a) Compute the forward pass to get $L$\n",
    "\n",
    "b) Compute $\\frac{\\partial L}{\\partial w_1}$ using the chain rule\n",
    "\n",
    "c) Update $w_1$ using gradient descent with $\\alpha = 0.1$\n",
    "\n",
    "d) Verify numerically using $\\frac{\\partial L}{\\partial w_1} \\approx \\frac{L(w_1 + h) - L(w_1)}{h}$\n",
    "\n",
    "---\n",
    "\n",
    "**Check your answers in the next cell!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### ğŸ¯ Core Concepts Mastered\n",
    "\n",
    "#### 1. **Derivative Definition**\n",
    "- **Limit definition:** $f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$\n",
    "- **Interpretation:** Instantaneous rate of change, slope of tangent line\n",
    "- **Connection:** Limits â†’ Derivatives â†’ Foundation of calculus\n",
    "\n",
    "#### 2. **Differentiation Rules**\n",
    "\n",
    "| Rule | Formula | When to Use |\n",
    "|------|---------|-------------|\n",
    "| Power Rule | $(x^n)' = nx^{n-1}$ | Polynomials, power functions |\n",
    "| Product Rule | $(fg)' = f'g + fg'$ | Product of two functions |\n",
    "| Quotient Rule | $\\left(\\frac{f}{g}\\right)' = \\frac{f'g - fg'}{g^2}$ | Ratio of functions |\n",
    "| Chain Rule | $(f \\circ g)' = f'(g(x)) \\cdot g'(x)$ | Composite functions |\n",
    "\n",
    "**Mnemonic for Quotient Rule:** \"Low d-high minus high d-low, over low-low\"\n",
    "\n",
    "#### 3. **Critical Points and Extrema**\n",
    "\n",
    "**Finding Critical Points:**\n",
    "1. Compute $f'(x)$\n",
    "2. Solve $f'(x) = 0$\n",
    "3. Check where $f'(x)$ is undefined\n",
    "\n",
    "**Classification Methods:**\n",
    "\n",
    "**First Derivative Test:**\n",
    "- $f'$ changes + to âˆ’ â†’ **Local Maximum**\n",
    "- $f'$ changes âˆ’ to + â†’ **Local Minimum**\n",
    "\n",
    "**Second Derivative Test:**\n",
    "- $f''(c) > 0$ â†’ **Local Minimum** (concave up âŒ£)\n",
    "- $f''(c) < 0$ â†’ **Local Maximum** (concave down âŒ¢)\n",
    "- $f''(c) = 0$ â†’ Inconclusive (use 1st derivative test)\n",
    "\n",
    "#### 4. **Optimization**\n",
    "\n",
    "**Gradient Descent Algorithm:**\n",
    "```\n",
    "Initialize: xâ‚€, Î± (learning rate)\n",
    "Repeat:\n",
    "  x_{k+1} = x_k - Î±Â·âˆ‡f(x_k)\n",
    "Until convergence\n",
    "```\n",
    "\n",
    "**Key Parameters:**\n",
    "- **Learning rate (Î±):** Too small = slow, too large = divergence\n",
    "- **Convergence criterion:** $|x_{k+1} - x_k| < \\epsilon$\n",
    "\n",
    "**Newton's Method:** Faster convergence using second derivatives\n",
    "- Update: $x_{k+1} = x_k - \\frac{f'(x_k)}{f''(x_k)}$\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Essential Formulas Reference\n",
    "\n",
    "#### Common Derivatives\n",
    "\n",
    "| Function | Derivative |\n",
    "|----------|------------|\n",
    "| $c$ (constant) | $0$ |\n",
    "| $x^n$ | $nx^{n-1}$ |\n",
    "| $e^x$ | $e^x$ |\n",
    "| $\\ln(x)$ | $\\frac{1}{x}$ |\n",
    "| $\\sin(x)$ | $\\cos(x)$ |\n",
    "| $\\cos(x)$ | $-\\sin(x)$ |\n",
    "| $\\tan(x)$ | $\\sec^2(x)$ |\n",
    "\n",
    "#### Chain Rule Examples\n",
    "- $(x^2 + 1)^5$ â†’ $5(x^2+1)^4 \\cdot 2x$\n",
    "- $e^{x^2}$ â†’ $e^{x^2} \\cdot 2x$\n",
    "- $\\sin(3x)$ â†’ $\\cos(3x) \\cdot 3$\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¤– Machine Learning Connections\n",
    "\n",
    "#### 1. **Gradient Descent = Core of Training**\n",
    "- Neural networks: Update weights using $w := w - \\alpha \\frac{\\partial L}{\\partial w}$\n",
    "- Linear regression: Minimize MSE using gradients\n",
    "- All ML optimization relies on computing derivatives!\n",
    "\n",
    "#### 2. **Backpropagation = Chain Rule**\n",
    "- Forward pass: Compute predictions\n",
    "- Backward pass: Apply chain rule to compute gradients\n",
    "- Example: $\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}$\n",
    "\n",
    "#### 3. **Loss Minimization**\n",
    "- **Objective:** Find parameters that minimize loss function\n",
    "- **Method:** Gradient descent or variants (SGD, Adam)\n",
    "- **Derivatives:** Tell us direction to move in parameter space\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”— Connections to Other Topics\n",
    "\n",
    "#### From Week 9 (Limits)\n",
    "- Derivatives **defined using limits**: $f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$\n",
    "- Continuity required for differentiability\n",
    "- Limit laws enable computing derivatives\n",
    "\n",
    "#### To Week 11 (Integration)\n",
    "- Integration is **inverse of differentiation**\n",
    "- Fundamental Theorem of Calculus links them\n",
    "- Applications: Area, accumulation, probability\n",
    "\n",
    "#### To Statistics\n",
    "- Probability density functions\n",
    "- Maximum likelihood estimation\n",
    "- Normal distribution derived using derivatives\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ Problem-Solving Strategy\n",
    "\n",
    "#### Differentiation Problems:\n",
    "1. **Identify** which rule(s) to apply\n",
    "2. **Apply** rules systematically (inside-out for chain rule)\n",
    "3. **Simplify** the result\n",
    "4. **Verify** using SymPy or numerical methods\n",
    "\n",
    "#### Optimization Problems:\n",
    "1. **Define** objective function $f(x)$\n",
    "2. **Compute** $f'(x)$ and find critical points\n",
    "3. **Classify** using second derivative test\n",
    "4. **Check** endpoints if on closed interval\n",
    "5. **Verify** answer makes sense in context\n",
    "\n",
    "#### ML Applications:\n",
    "1. **Define** loss function $L(\\theta)$\n",
    "2. **Compute** gradient $\\nabla L(\\theta)$\n",
    "3. **Initialize** parameters\n",
    "4. **Iterate** gradient descent: $\\theta := \\theta - \\alpha \\nabla L$\n",
    "5. **Monitor** convergence\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Self-Assessment Checklist\n",
    "\n",
    "Check off each item you can confidently do:\n",
    "\n",
    "- [ ] Compute derivatives using limit definition\n",
    "- [ ] Apply power rule to polynomials\n",
    "- [ ] Use product rule for products of functions\n",
    "- [ ] Apply quotient rule correctly\n",
    "- [ ] Use chain rule for composite functions\n",
    "- [ ] Find critical points by solving $f'(x) = 0$\n",
    "- [ ] Classify critical points using first derivative test\n",
    "- [ ] Classify critical points using second derivative test\n",
    "- [ ] Find global extrema on closed intervals\n",
    "- [ ] Set up and solve optimization word problems\n",
    "- [ ] Implement gradient descent algorithm\n",
    "- [ ] Understand connection to machine learning\n",
    "- [ ] Apply derivatives to related rates problems\n",
    "- [ ] Use implicit differentiation\n",
    "- [ ] Understand backpropagation as chain rule\n",
    "\n",
    "**Goal:** Check all boxes! If not, review relevant sections.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š Additional Resources\n",
    "\n",
    "#### For Deeper Understanding:\n",
    "- **3Blue1Brown:** \"Essence of Calculus\" YouTube series (visual intuition)\n",
    "- **Khan Academy:** Calculus I and optimization\n",
    "- **MIT OCW:** Single Variable Calculus (18.01)\n",
    "\n",
    "#### For ML Applications:\n",
    "- **Andrew Ng:** Machine Learning course (Coursera)\n",
    "- **Deep Learning Book:** Chapter 4 (Numerical Computation)\n",
    "- **Fast.ai:** Practical Deep Learning\n",
    "\n",
    "#### Practice Problems:\n",
    "- **Paul's Online Math Notes:** Calculus I\n",
    "- **Stewart Calculus:** Classic textbook exercises\n",
    "- **Kaggle:** ML competitions applying optimization\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ What's Next?\n",
    "\n",
    "#### Week 11: Integration\n",
    "- Antiderivatives and indefinite integrals\n",
    "- Definite integrals and area under curves\n",
    "- Fundamental Theorem of Calculus\n",
    "- Applications to probability and statistics\n",
    "\n",
    "#### Weeks 12: Advanced Integration\n",
    "- Integration by substitution\n",
    "- Integration by parts\n",
    "- Numerical integration methods\n",
    "\n",
    "#### Connection to Data Science:\n",
    "- **Probability:** Integration for continuous distributions\n",
    "- **Statistics:** Maximum likelihood estimation\n",
    "- **Machine Learning:** Loss functions, regularization\n",
    "- **Optimization:** Constrained optimization, Lagrange multipliers\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ‰ Congratulations!\n",
    "\n",
    "You've completed **Week 10: Derivatives**!\n",
    "\n",
    "You now understand:\n",
    "- âœ… How to compute derivatives using multiple methods\n",
    "- âœ… How to find and classify critical points\n",
    "- âœ… How optimization algorithms work\n",
    "- âœ… The mathematical foundation of machine learning\n",
    "- âœ… How calculus powers modern AI and data science\n",
    "\n",
    "**Derivatives are everywhere in data science and machine learning. You've just learned one of the most powerful tools in mathematics!**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Final Notes\n",
    "\n",
    "**Key Insight:** Derivatives measure **rates of change**. In ML, we use derivatives to measure how loss changes with parameters, allowing us to optimize models.\n",
    "\n",
    "**Practice Tip:** The best way to master derivatives is to solve many problems. Work through the practice problems, verify with code, and build intuition.\n",
    "\n",
    "**Looking Ahead:** Integration (Week 11) completes the calculus foundation. Together, derivatives and integrals form the language of change and accumulationâ€”essential for understanding probability, statistics, and advanced ML.\n",
    "\n",
    "**Keep Learning!** ğŸš€ğŸ“ŠğŸ¤–"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
