{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1439061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import TYPE_CHECKING\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from pandas import DataFrame, Series\n",
    "\n",
    "np.random.seed(42)\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263a4e6a",
   "metadata": {},
   "source": [
    "## 1. Orthogonality\n",
    "### 1.1 Theory\n",
    "Orthogonal vectors are perpendicular - their inner product is zero. Orthonormal vectors are orthogonal with unit length.\n",
    "\n",
    "### 1.2 Mathematical Definition\n",
    "- **Orthogonal**: $\\langle \\mathbf{u}, \\mathbf{v} \\rangle = 0$\n",
    "- **Orthonormal**: Orthogonal and $\\|\\mathbf{u}\\| = \\|\\mathbf{v}\\| = 1$\n",
    "- **Orthogonal matrix**: $Q^TQ = QQ^T = I$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eb65ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Orthogonality - Implementation\n",
    "# TODO: Add orthogonality checks\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2fca15",
   "metadata": {},
   "source": [
    "### 1.3 Supply Chain Application\n",
    "**Retail Context**: Orthogonal features are uncorrelated and provide independent information. PCA produces orthogonal principal components for maximum variance capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f4301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supply Chain Example: Uncorrelated Features\n",
    "# TODO: Add feature orthogonality analysis\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0d2c69",
   "metadata": {},
   "source": [
    "## 2. Gram-Schmidt Process\n",
    "### 2.1 Theory\n",
    "Gram-Schmidt transforms any basis into an orthonormal basis. This is essential for QR decomposition and stable numerical computations.\n",
    "\n",
    "### 2.2 Mathematical Definition\n",
    "Given $\\{\\mathbf{v}_1, ..., \\mathbf{v}_n\\}$, produce $\\{\\mathbf{e}_1, ..., \\mathbf{e}_n\\}$:\n",
    "\n",
    "$$\\mathbf{u}_k = \\mathbf{v}_k - \\sum_{j=1}^{k-1} \\text{proj}_{\\mathbf{u}_j}(\\mathbf{v}_k)$$\n",
    "$$\\mathbf{e}_k = \\frac{\\mathbf{u}_k}{\\|\\mathbf{u}_k\\|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bfa29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Gram-Schmidt - Implementation\n",
    "# TODO: Add Gram-Schmidt algorithm\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa37710",
   "metadata": {},
   "source": [
    "### 2.3 Supply Chain Application\n",
    "**Retail Context**: Gram-Schmidt underlies QR decomposition used in regression. It creates stable numerical solutions for demand forecasting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7878d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supply Chain Example: QR for Regression\n",
    "# TODO: Add QR decomposition example\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e626c4",
   "metadata": {},
   "source": [
    "## 3. Orthogonal Projections\n",
    "### 3.1 Theory\n",
    "Projecting onto a subspace finds the closest point in that subspace. This is the geometric foundation of least squares.\n",
    "\n",
    "### 3.2 Mathematical Definition\n",
    "Projection of $\\mathbf{b}$ onto column space of $A$:\n",
    "$$\\text{proj}_A(\\mathbf{b}) = A(A^TA)^{-1}A^T\\mathbf{b}$$\n",
    "\n",
    "**Least Squares**: $\\hat{\\mathbf{x}} = (A^TA)^{-1}A^T\\mathbf{b}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331abd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Projections - Implementation\n",
    "# TODO: Add projection computations\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e513783",
   "metadata": {},
   "source": [
    "### 3.3 Supply Chain Application\n",
    "**Retail Context**: Least squares regression for demand forecasting is an orthogonal projection. The fitted values are projections of observed demand onto the model space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc44b26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supply Chain Example: Least Squares Forecasting\n",
    "# TODO: Add demand forecasting regression\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f379c6cb",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "1. **Exercise 1**: Apply Gram-Schmidt to three feature vectors to create orthonormal features.\n",
    "2. **Exercise 2**: Project a demand vector onto the subspace spanned by seasonal patterns.\n",
    "3. **Exercise 3**: Solve a least squares regression problem using the normal equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0fbc31",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Orthogonal vectors have zero inner product\n",
    "- Gram-Schmidt produces orthonormal bases\n",
    "- Projections find closest points in subspaces\n",
    "- Least squares is projection onto model space\n",
    "\n",
    "## Next Week Preview\n",
    "Week 9 covers **Multivariable Functions and Partial Derivatives**.\n",
    "\n",
    "---\n",
    "*IIT Madras BS Degree in Data Science*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
