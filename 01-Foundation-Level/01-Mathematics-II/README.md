# BSMA1003: Mathematics for Data Science II

**Course ID:** BSMA1003
**Credits:** 4
**Duration:** 11 weeks
**Level:** Foundation
**Prerequisites:** BSMA1001 - Mathematics for Data Science I

---

## ğŸ“– Course Overview

Mathematics for Data Science II focuses on linear algebra, multivariable calculus, and optimization techniques essential for machine learning and data science. This course provides the mathematical foundation for understanding and implementing advanced ML algorithms, dimensionality reduction, and optimization methods.

**Key Topics:**
- Linear algebra (matrices, vector spaces, eigenvalues)
- Multivariable calculus (gradients, partial derivatives)
- Optimization (gradient descent, Lagrange multipliers)
- Applications to machine learning (PCA, linear regression, neural networks)

---

## ğŸ¯ Learning Objectives

By the end of this course, you will be able to:
- Manipulate matrices and solve systems of linear equations
- Understand vector spaces, linear independence, and bases
- Calculate distances and angles using norms and inner products
- Apply Gram-Schmidt orthogonalization and QR decomposition
- Compute gradients and Hessians for optimization
- Solve optimization problems with and without constraints
- Apply linear algebra concepts to machine learning problems
- Implement fundamental ML algorithms using linear algebra

---

## ğŸ“š Course Structure

### Module 1: Linear Algebra Foundations (Weeks 1-4)
- **Week 1:** Vectors and Matrices
- **Week 2:** Solving Linear Equations
- **Week 3:** Vector Spaces and Linear Independence
- **Week 4:** Basis and Dimension

### Module 2: Advanced Linear Algebra (Weeks 5-6)
- **Week 5:** Norms and Inner Products
- **Week 6:** Gram-Schmidt Orthogonalization

### Module 3: Calculus and Optimization (Weeks 7-9)
- **Week 7:** Single Variable Calculus - Derivatives
- **Week 8:** Multivariable Calculus - Partial Derivatives
- **Week 9:** Optimization - Finding Extrema

### Module 4: Machine Learning Applications (Weeks 10-11)
- **Week 10:** Applications to Machine Learning
- **Week 11:** Review and Advanced Topics

---

## ğŸ“‚ Repository Structure

```
01-Mathematics-II/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ study-guide.md              # Detailed 11-week study plan
â”œâ”€â”€ notebook-template-guide.md  # Notebook creation standards
â”œâ”€â”€ notes/                       # ğŸ“ Theory & Concepts (Markdown)
â”‚   â”œâ”€â”€ 00-bsma1003-overview.md             âœ…
â”‚   â”œâ”€â”€ week-01-vectors-matrices-intro.md   âœ…
â”‚   â”œâ”€â”€ week-02-matrix-operations.md        âœ…
â”‚   â”œâ”€â”€ week-03-linear-equations-systems.md âœ…
â”‚   â”œâ”€â”€ week-04-determinants.md             âœ…
â”‚   â”œâ”€â”€ week-05-gaussian-elimination.md     âœ…
â”‚   â”œâ”€â”€ week-06-vector-spaces.md            âœ…
â”‚   â”œâ”€â”€ week-07-basis-dimension.md          âœ…
â”‚   â”œâ”€â”€ week-08-rank-nullity.md             âœ…
â”‚   â”œâ”€â”€ week-09-optimization-basics.md      âœ…
â”‚   â”œâ”€â”€ week-10-ml-applications.md          âœ…
â”‚   â””â”€â”€ week-11-advanced-topics.md          âœ…
â”œâ”€â”€ notebooks/                   # ğŸ’» Implementation & Practice (Jupyter)
â”‚   â”œâ”€â”€ week-01-practice.ipynb              âœ…
â”‚   â”œâ”€â”€ week-02-practice.ipynb              âœ…
â”‚   â”œâ”€â”€ week-03-practice.ipynb              âœ…
â”‚   â”œâ”€â”€ week-04-determinants.ipynb          âœ…
â”‚   â”œâ”€â”€ week-05-gaussian-elimination.ipynb  âœ…
â”‚   â”œâ”€â”€ week-06-vector-spaces.ipynb         âœ…
â”‚   â”œâ”€â”€ week-07-basis-dimension.ipynb       âœ…
â”‚   â”œâ”€â”€ week-08-rank-nullity.ipynb          âœ…
â”‚   â”œâ”€â”€ week-09-optimization-basics.ipynb   âœ…
â”‚   â”œâ”€â”€ week-10-ml-applications.ipynb       âœ…
â”‚   â””â”€â”€ week-11-advanced-topics.ipynb       âœ…
â”œâ”€â”€ assignments/                 # Course assignments
â”œâ”€â”€ practice/                    # Additional exercises
â””â”€â”€ resources/                   # Reference materials
```

---

## âœ… Weekly Progress Tracker

### Module 1: Linear Algebra Foundations
- [x] **Week 1:** Vectors and Matrices âœ…
  - Notes: `week-01-vectors-matrices-intro.md`
  - Notebook: `week-01-practice.ipynb`
  - Topics: Vector operations, matrix multiplication, determinants

- [x] **Week 2:** Solving Linear Equations âœ…
  - Notes: `week-02-matrix-operations.md`
  - Notebook: `week-02-practice.ipynb`
  - Topics: Gaussian elimination, matrix inverse, Cramer's rule

- [x] **Week 3:** Vector Spaces âœ…
  - Notes: `week-03-linear-equations-systems.md`
  - Notebook: `week-03-practice.ipynb`
  - Topics: Vector space axioms, subspaces, linear independence

- [x] **Week 4:** Basis and Dimension âœ…
  - Notes: `week-04-determinants.md`
  - Notebook: `week-04-determinants.ipynb`
  - Topics: Basis, dimension, rank, rank-nullity theorem

### Module 2: Advanced Linear Algebra
- [x] **Week 5:** Norms and Inner Products âœ…
  - Notes: `week-05-gaussian-elimination.md`
  - Notebook: `week-05-gaussian-elimination.ipynb`
  - Topics: L1/L2 norms, dot products, orthogonality

- [x] **Week 6:** Gram-Schmidt Orthogonalization âœ…
  - Notes: `week-06-vector-spaces.md`
  - Notebook: `week-06-vector-spaces.ipynb`
  - Topics: Orthonormal bases, QR decomposition

### Module 3: Calculus and Optimization
- [x] **Week 7:** Single Variable Calculus âœ…
  - Notes: `week-07-basis-dimension.md`
  - Notebook: `week-07-basis-dimension.ipynb`
  - Topics: Derivatives, critical points, optimization

- [x] **Week 8:** Multivariable Calculus âœ…
  - Notes: `week-08-rank-nullity.md`
  - Notebook: `week-08-rank-nullity.ipynb`
  - Topics: Partial derivatives, gradients, directional derivatives

- [x] **Week 9:** Optimization âœ…
  - Notes: `week-09-optimization-basics.md`
  - Notebook: `week-09-optimization-basics.ipynb`
  - Topics: Hessian, Lagrange multipliers, convex functions

### Module 4: Machine Learning Applications
- [x] **Week 10:** ML Applications âœ…
  - Notes: `week-10-ml-applications.md`
  - Notebook: `week-10-ml-applications.ipynb`
  - Topics: Linear regression, gradient descent, PCA

- [x] **Week 11:** Review and Advanced Topics âœ…
  - Notes: `week-11-advanced-topics.md`
  - Notebook: `week-11-advanced-topics.ipynb`
  - Topics: Matrix decompositions, numerical methods

**Overall Progress:** 11/11 weeks complete (100%) âœ…

---

## ğŸ¯ Assessment Structure

- **Weekly Online Assignments:** 10-20%
- **Quiz 1 (In-person):** 15-20%
- **Quiz 2 (In-person):** 15-20%
- **End Term Exam (In-person):** 50-60%

**Passing Criteria:**
- Overall score: â‰¥40%
- End-term exam: â‰¥40%

---

## ğŸ’¡ Study Tips & Strategies

### Daily Practice (1-2 hours)
1. **Theory:** Read notes and textbook (30 min)
2. **Practice:** Solve problems manually (30 min)
3. **Code:** Implement in Python/NumPy (30 min)
4. **Visualize:** Create plots to understand concepts (15 min)

### Weekly Workflow
1. **Monday:** Watch video lectures, read study guide
2. **Tuesday-Thursday:** Complete notes, practice problems
3. **Friday:** Create/complete Jupyter notebook
4. **Weekend:** Complete assignment, review concepts

### Key Success Strategies
- âœ… **Master NumPy:** Every concept should be implemented in code
- âœ… **Visualize Everything:** Use matplotlib for matrices, vectors, transformations
- âœ… **Connect to ML:** Always ask "How is this used in machine learning?"
- âœ… **Build Intuition:** Don't just memorize - understand geometric meaning
- âœ… **Practice Daily:** Small consistent practice beats cramming
- âœ… **Form Study Groups:** Explain concepts to solidify understanding

---

## ğŸ“Š Key Python Libraries

```python
import numpy as np                    # Matrix operations, linear algebra
import matplotlib.pyplot as plt       # Visualization
from scipy.linalg import qr, svd     # Advanced linear algebra
from scipy.optimize import minimize   # Optimization
import sympy as sp                    # Symbolic mathematics
```

### Essential NumPy Operations
```python
# Matrix operations
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])
C = A @ B                        # Matrix multiplication
A_inv = np.linalg.inv(A)        # Matrix inverse
det_A = np.linalg.det(A)        # Determinant
rank_A = np.linalg.matrix_rank(A)  # Rank

# Eigenvalues and eigenvectors
eigenvalues, eigenvectors = np.linalg.eig(A)

# Norms
l2_norm = np.linalg.norm(v)     # Euclidean norm
l1_norm = np.linalg.norm(v, ord=1)  # Manhattan norm

# QR decomposition
Q, R = np.linalg.qr(A)

# SVD
U, s, Vt = np.linalg.svd(A)
```

---

## ğŸ”— Resources

### Required Materials
- **Linear Algebra Textbook:** [Download PDF](https://drive.google.com/file/d/1nMGSpQfLObffDsaojI-56kkl_tuo94hJ/view)
- **Video Lectures:** [YouTube Playlist](https://www.youtube.com/playlist?list=PLZ2ps__7DhBboGlwPVSsWP8loAJCLrKc8)
- **Course Page:** [IIT Madras Study Portal](https://study.iitm.ac.in/ds/course_pages/BSMA1003.html)

### Additional Resources

**Books:**
- *Mathematics for Machine Learning* - [Free PDF](https://mml-book.github.io/book/mml-book.pdf)
- *Linear Algebra and Its Applications* by Gilbert Strang
- *Introduction to Linear Algebra* by Gilbert Strang

**Video Resources:**
- [3Blue1Brown - Essence of Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) - Visual intuition
- [MIT OCW 18.06 - Linear Algebra](https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/) - Gilbert Strang's legendary course
- [Khan Academy - Linear Algebra](https://www.khanacademy.org/math/linear-algebra)

**Interactive Tools:**
- [Matrix Calculator](https://www.wolframalpha.com/) - WolframAlpha
- [Matrix Visualizations](https://www.geogebra.org/m/YtYSC4Zg)
- [NumPy Documentation](https://numpy.org/doc/stable/reference/routines.linalg.html)

**Practice Problems:**
- MIT OCW Problem Sets
- [Linear Algebra Practice](https://www.khanacademy.org/math/linear-algebra)
- Past IIT Madras assignments (check course page)

---

## ğŸ“ Prerequisites Review

Before starting this course, ensure you're comfortable with:
- âœ… Basic algebra and equation solving
- âœ… Functions and graphing
- âœ… Polynomial operations
- âœ… Basic calculus (derivatives and integrals)
- âœ… Python programming basics
- âœ… NumPy fundamentals

**Need Review?** See `../01-Mathematics-I/` for prerequisite materials.

---

## ğŸ” Key Concepts Quick Reference

### Linear Algebra
- **Matrix:** Rectangular array of numbers representing linear transformations
- **Determinant:** Scalar indicating if matrix is invertible
- **Rank:** Number of linearly independent rows/columns
- **Eigenvalue:** Î» such that Av = Î»v
- **Basis:** Linearly independent set spanning a vector space

### Calculus
- **Gradient:** Vector of partial derivatives âˆ‡f = [âˆ‚f/âˆ‚xâ‚, âˆ‚f/âˆ‚xâ‚‚, ...]
- **Hessian:** Matrix of second-order partial derivatives
- **Critical Point:** Where gradient = 0
- **Convex Function:** Second derivative â‰¥ 0 everywhere

### Optimization
- **Gradient Descent:** Iterative optimization: Î¸ := Î¸ - Î±âˆ‡f(Î¸)
- **Lagrange Multipliers:** Method for constrained optimization
- **Learning Rate:** Step size Î± in gradient descent

---

## ğŸ“ˆ Real-World Applications

This course provides mathematical foundations for:

### Machine Learning
- **Linear Regression:** Solving Ax = b using normal equations
- **Neural Networks:** Gradient descent for weight optimization
- **PCA:** Eigenvalue decomposition for dimensionality reduction
- **SVD:** Recommender systems, image compression

### Data Science
- **Feature Scaling:** Using norms and distances
- **Clustering:** Distance metrics (L1, L2)
- **Regularization:** L1 (Lasso) and L2 (Ridge) in regression
- **Optimization:** Training all ML models

### Computer Vision
- **Image Processing:** Matrix operations on pixel data
- **Face Recognition:** Eigenfaces using PCA
- **3D Graphics:** Transformation matrices

---

## ğŸš€ Getting Started

### Week 1 Quick Start

1. **Setup Environment:**
   ```bash
   source .venv/bin/activate  # Activate virtual environment
   jupyter lab                 # Start Jupyter
   ```

2. **Read Materials:**
   - Start with `study-guide.md` Week 1 section
   - Read `notes/week-01-vectors-matrices-intro.md`

3. **Watch Lectures:**
   - Week 1 videos from YouTube playlist
   - Take notes while watching

4. **Practice:**
   - Open `notebooks/week-01-practice.ipynb`
   - Run all cells, modify code, experiment
   - Try additional problems

5. **Complete Assignment:**
   - Check course portal for Week 1 assignment
   - Submit before deadline

---

## ğŸ“ Notes vs Notebooks

**Notes** (`notes/` directory):
- ğŸ“ Theory, definitions, theorems, formulas
- ğŸ“– Worked examples with step-by-step solutions
- ğŸ”— Cross-references and connections
- ğŸ“Š Mathematical proofs
- **Format:** Markdown with LaTeX

**Notebooks** (`notebooks/` directory):
- ğŸ’» Python implementations
- ğŸ“ˆ Visualizations and plots
- ğŸ§ª Experiments and exploration
- âœ… Practice problems with solutions
- **Format:** Jupyter Notebook (`.ipynb`)

**Both are essential:** Theory gives understanding, practice gives skill.

---

## ğŸ¯ Learning Path

### Beginner (Weeks 1-4)
Focus on building solid foundations:
- Master matrix operations
- Understand vector spaces
- Practice solving systems manually and with code

### Intermediate (Weeks 5-7)
Deepen understanding:
- Calculate norms and distances
- Apply orthogonalization
- Compute derivatives and optimize functions

### Advanced (Weeks 8-11)
Apply to real problems:
- Implement gradient descent
- Build PCA from scratch
- Solve optimization problems
- Complete ML applications

---

## âœ¨ Success Checklist

By the end of this course, you should be able to:

### Linear Algebra
- [ ] Multiply matrices and compute inverses
- [ ] Solve systems using Gaussian elimination
- [ ] Determine linear independence of vectors
- [ ] Find basis and dimension of vector spaces
- [ ] Calculate norms and inner products
- [ ] Apply Gram-Schmidt orthogonalization
- [ ] Compute eigenvalues and eigenvectors
- [ ] Perform SVD decomposition

### Calculus
- [ ] Find derivatives of multivariable functions
- [ ] Compute gradients and Hessians
- [ ] Identify critical points
- [ ] Classify extrema using second derivative test
- [ ] Apply Lagrange multipliers

### Machine Learning
- [ ] Implement linear regression from scratch
- [ ] Code gradient descent algorithm
- [ ] Build PCA for dimensionality reduction
- [ ] Apply regularization (L1/L2)
- [ ] Understand matrix operations in neural networks

### Programming
- [ ] Use NumPy for all linear algebra operations
- [ ] Visualize matrices and transformations
- [ ] Implement optimization algorithms
- [ ] Debug numerical issues
- [ ] Write efficient vectorized code

---

## ğŸ† Next Steps

After completing BSMA1003:
1. **Apply to ML:** Take Machine Learning courses
2. **Deepen Math:** Study advanced optimization, numerical methods
3. **Build Projects:** Implement ML algorithms from scratch
4. **Explore Further:**
   - Tensor calculus (for deep learning)
   - Convex optimization
   - Numerical linear algebra
   - Differential equations

---

## ğŸ“§ Getting Help

- **Discussion Forum:** Course-specific forum on IIT Madras portal
- **Office Hours:** Check course page for instructor availability
- **Study Group:** Form/join groups with fellow students
- **Stack Overflow:** For Python/NumPy specific questions
- **GitHub Issues:** Report problems with this repository

---

**Last Updated:** November 22, 2025
**Status:** All 11 weeks complete âœ…
**Next Course:** BSMA1004 - Statistics for Data Science II

---

*Remember: Linear algebra is the language of machine learning. Master these concepts and you'll understand the mathematics behind modern AI!* ğŸš€
